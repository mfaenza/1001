{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import random\n",
    "\n",
    "ua = UserAgent() # From here we generate a random user agent\n",
    "proxies = [] # Will contain proxies [ip, port]\n",
    "\n",
    "# Retrieve latest proxies\n",
    "proxies_req = Request('https://www.sslproxies.org/')\n",
    "proxies_req.add_header('User-Agent', ua.random)\n",
    "proxies_doc = urlopen(proxies_req).read().decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(proxies_doc, 'html.parser')\n",
    "proxies_table = soup.find(id='proxylisttable')\n",
    "\n",
    "# Retrieve a random index proxy (we need the index to delete it if not working)\n",
    "def random_proxy():\n",
    "    return random.randint(0, len(proxies) - 1)\n",
    "\n",
    "# Save proxies in the array\n",
    "for row in proxies_table.tbody.find_all('tr'):\n",
    "    proxies.append({\n",
    "    'ip':   row.find_all('td')[0].string,\n",
    "    'port': row.find_all('td')[1].string\n",
    "    })\n",
    "\n",
    "print('starting')\n",
    "proxy_index = random_proxy()\n",
    "proxy = proxies[proxy_index]\n",
    "for n in range(1, 100):\n",
    "    \n",
    "    print(n)\n",
    "    req = Request('https://www.1001tracklists.com/source/v7m7k3/the-anjunadeep-edition/index.html')\n",
    "    req.set_proxy(proxy['ip'] + ':' + proxy['port'], 'http')\n",
    "\n",
    "    # Every 10 requests, generate a new proxy\n",
    "    if n % 10 == 0:\n",
    "        proxy_index = random_proxy()\n",
    "        proxy = proxies[proxy_index]\n",
    "        \n",
    "    print('making call')\n",
    "    # Make the call\n",
    "    try:\n",
    "        my_ip = urlopen(req, timeout=5).read().decode('utf8')\n",
    "        print('#' + str(n))\n",
    "    except Exception as e: # If error, delete this proxy and find another one\n",
    "        print(e)\n",
    "        del proxies[proxy_index]\n",
    "        print('Proxy ' + proxy['ip'] + ':' + proxy['port'] + ' deleted.')\n",
    "        proxy_index = random_proxy()\n",
    "        proxy = proxies[proxy_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "myclient = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "\n",
    "mydb = myclient['1001']\n",
    "print(len(list(mydb['track_docs'].find({}))))\n",
    "print(len(list(mydb['artist_docs'].find({}))))\n",
    "print(len(list(mydb['played_docs'].find({}))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parser Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import Parser\n",
    "        \n",
    "prsr = Parser.Parser()\n",
    "prsr.parse(url='https://www.1001tracklists.com/tracklist/2rrcqmpk/modd-vulcan-gas-company-austin-united-states-2019-03-09.html',\\\n",
    "           html=prsr.request('https://www.1001tracklists.com/tracklist/2rrcqmpk/modd-vulcan-gas-company-austin-united-states-2019-03-09.html'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawler Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth: 0\n",
      "https://www.1001tracklists.com/source/7x3gmv/afterlife-voyage/index.html\n",
      "Depth: 1\n",
      "Depth: 1\n",
      "Depth: 1\n",
      "Depth: 1\n",
      "Depth: 1\n",
      "Depth: 1\n",
      "Depth: 1\n",
      "Depth: 1\n",
      "Depth: 1\n",
      "Depth: 1\n",
      "Depth: 1\n",
      "https://www.1001tracklists.com/tracklist/25152ntk/vaal-afterlife-voyage-002-2017-01-19.html\n",
      "Len tracklist docs: 1\n",
      "Len played docs: 14\n",
      "Len sequential docs: 16\n",
      "Len track docs: 136\n",
      "Len artist docs: 85\n",
      "Depth: 2\n",
      "https://www.1001tracklists.com/dj/vaal/index.html\n",
      "Depth: 3\n",
      "Depth: 3\n",
      "https://www.1001tracklists.com/tracklist/1s0vb4jk/vaal-eb-radio-dj-mix-2017-09-03.html\n",
      "Len tracklist docs: 2\n",
      "Len played docs: 22\n",
      "Len sequential docs: 26\n",
      "Len track docs: 191\n",
      "Len artist docs: 111\n",
      "Depth: 4\n",
      "Depth: 4\n",
      "Depth: 3\n",
      "https://www.1001tracklists.com/tracklist/2gcnfsw1/vaal-ants-stage-tomorrowland-weekend-2-belgium-2017-07-30.html\n",
      "Len tracklist docs: 3\n",
      "Len played docs: 30\n",
      "Len sequential docs: 37\n",
      "Len track docs: 257\n",
      "Len artist docs: 137\n",
      "Depth: 4\n",
      "Depth: 4\n",
      "Depth: 3\n",
      "Depth: 2\n",
      "Depth: 1\n"
     ]
    }
   ],
   "source": [
    "import Crawler\n",
    "import numpy as np\n",
    "\n",
    "crawler = Crawler.Crawler(max_depth=20, batch_limit=np.inf)\n",
    "crawler.start_crawl('https://www.1001tracklists.com/source/7x3gmv/afterlife-voyage/index.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ParserSandbox\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ftfy\n",
    "import urllib\n",
    "from urllib.request import Request, urlopen\n",
    "\n",
    "def find_str(s, char, start_index=0):\n",
    "    '''\n",
    "    Find substring char in string s. Found on internet, probably not efficient.\n",
    "    \n",
    "    '''\n",
    "    index = 0\n",
    "    s = s[start_index+1:]\n",
    "    if char in s:\n",
    "        c = char[0]\n",
    "        for ch in s:\n",
    "            if ch == c:\n",
    "                if s[index:index+len(char)] == char:\n",
    "                    return start_index + 1 + index\n",
    "            index += 1\n",
    "    return -1\n",
    "\n",
    "def extract_value(html, key_value):\n",
    "    \n",
    "    content_string = html.strip(key_value).split('>')[0].strip('\"')\n",
    "    return content_string\n",
    "\n",
    "def fix_decoding_errors(string):\n",
    "    '''\n",
    "    Fix UTF-8 decoding issues. Probably need to find more systematic/thorough approach to this.\n",
    "    \n",
    "    REPLACE THIS WITH ftfy.fix_text() -- python package which should be one stop shop for fixes\n",
    "    '''\n",
    "    string = string.replace('&amp;','&')\n",
    "    string = string.replace('&#39;',\"'\")\n",
    "    string = string.replace('\\\\xc3\\\\xb6','o')\n",
    "    string = string.replace('\\\\xc3\\\\xab','e')\n",
    "    string = string.replace('\\\\xc3\\\\x9','u')\n",
    "    string = string.replace('\\\\xc3\\\\xb8','o')\n",
    "    string = string.replace(\"\\\\'\",\"'\")\n",
    "    \n",
    "    return ftfy.fix_text(string)\n",
    "\n",
    "def parse_track_and_artist(track_string):\n",
    "    '''\n",
    "    Extract the artist, track name, and remixer (if any) from the standard formatting used by 1001.\n",
    "    \n",
    "    '''\n",
    "    # Check if Remix/Bootleg/Edit and parse accordingly\n",
    "    if ('Remix' in track_string) or ('Bootleg' in track_string) or ('Edit' in track_string):\n",
    "        \n",
    "        artist, track_remixer = [string.strip(' ') for string in track_string.split(' - ')]\n",
    "        track_remixer = [string.strip(' ') for string in track_remixer.split('(')]\n",
    "        \n",
    "        if len(track_remixer) > 2:\n",
    "            track = track_remixer[0]\n",
    "            remixer = '('.join(track_remixer[1:])\n",
    "        else:\n",
    "            track, remixer = track_remixer\n",
    "            remixer = remixer.rstrip('Remix)').strip(' ')\n",
    "        \n",
    "    # If not remix, then should follow standard layout \"Artist Name - Track Name\"\n",
    "    # This layout is expressed explicitly in html\n",
    "    else:\n",
    "        \n",
    "        artist, track = [string.strip(' ') for string in track_string.split(' - ')]\n",
    "        remixer = 'N/A'\n",
    "\n",
    "    # Check for multiple artists -- Big Room sets tend to have hella mashups\n",
    "    # Sometimes there is more structured formatting to exploit i.e. (Artist1 vs. Artist2 - Track1 vs. Track2)\n",
    "    # Not worrying about that now b/c big room sux\n",
    "    if 'vs.' in artist:\n",
    "        artist = artist.replace('vs.','&')\n",
    "    if '&' in artist:\n",
    "        artist = [a.strip(' ') for a in artist.split('&')]\n",
    "    \n",
    "    # Remove features\n",
    "    # We could make features a separate field but for now just removing\n",
    "    if isinstance(artist, str):\n",
    "        if ('feat.' in artist) or ('ft.' in artist):\n",
    "            artist = artist.split('feat.')[0].strip(' ')\n",
    "            artist = artist.split('ft.')[0].strip(' ')\n",
    "    if isinstance(artist, list):\n",
    "            artist = [a.split('feat.')[0].split('ft.')[0].strip(' ') for a in artist]\n",
    "        \n",
    "    if isinstance(artist, list):\n",
    "        return (artist, track, remixer)\n",
    "    else:\n",
    "        return ([artist], track, remixer)\n",
    "\n",
    "\n",
    "def tracklist_meta_data(html):\n",
    "    '''\n",
    "    Extract meta data about tracklist/set.\n",
    "    \n",
    "    '''\n",
    "    meta_data = {}\n",
    "    \n",
    "    # Extract set description\n",
    "    index = 0\n",
    "    start_term = 'meta name=\"description\" content=\"'\n",
    "    index = find_str(html, start_term, index)\n",
    "    description = html[index:].split('>')[0]\n",
    "    description = description.lstrip(start_term).rstrip('\"')\n",
    "    meta_data['description'] = description\n",
    "    \n",
    "    # Set creation date - This should probably be the point in time we use for building prediction data\n",
    "    index = 0\n",
    "    start_term = 'meta name=\"dcterms.created\" content=\"'\n",
    "    index = find_str(html, start_term, index)\n",
    "    created = html[index:].split('>')[0]\n",
    "    created = created.lstrip(start_term).rstrip('\"')\n",
    "    meta_data['created'] = created\n",
    "    \n",
    "    # Set last modified data\n",
    "    index = 0\n",
    "    start_term = 'meta name=\"dcterms.modified\" content=\"'\n",
    "    index = find_str(html, start_term, index)\n",
    "    modified = html[index:].split('>')[0]\n",
    "    modified = modified.lstrip(start_term).rstrip('\"')\n",
    "    meta_data['modified'] = modified\n",
    "    \n",
    "    return meta_data\n",
    "    \n",
    "def tracklist_general_information(html):\n",
    "    '''\n",
    "    Extract general info about tracklist/set.\n",
    "    \n",
    "    '''\n",
    "    info_doc = {}\n",
    "    index = 0\n",
    "    start_term = 'General Information'\n",
    "    index = find_str(html, start_term, index)\n",
    "    info_chunk = html[index:].split('Most Liked Tracklists')[0]\n",
    "    \n",
    "    # Genres -- can use these to build genre-specific graphs\n",
    "    style_index = 0\n",
    "    style_index = find_str(info_chunk, 'Tracklist Musicstyle', style_index)\n",
    "    styles = info_chunk[style_index:].split('id=\"tl_music_styles\">')[1].split('</td>')[0]\n",
    "    styles = [style.strip(' ') for style in styles.split(',')]\n",
    "    info_doc['styles'] = styles\n",
    "    \n",
    "    # If 1001 recognizes the dj who played the set they link their dj page\n",
    "    # Its my understanding dj pages are independent of artist pages -- we'll need to map these\n",
    "    index = 0\n",
    "    start_term = 'a href=\"/dj'\n",
    "    index = find_str(html, start_term, index)\n",
    "    if index != -1:\n",
    "        dj_url = html[index:].split('class')[0].split('\"')[1]\n",
    "        dj_url = 'https://www.1001tracklists.com' + dj_url\n",
    "        info_doc['dj_url'] = dj_url\n",
    "\n",
    "        dj_name = html[index:].split('</a>')[0].split('>')[1]\n",
    "        info_doc['dj_name'] = dj_name\n",
    "    else:\n",
    "        info_doc['dj_url'] = 'N/A'\n",
    "        info_doc['dj_name'] = 'N/A'\n",
    "        \n",
    "    return info_doc\n",
    "    \n",
    "def tracklist_track_data(html):\n",
    "    '''\n",
    "    Extract track related data from set\n",
    "    '''\n",
    "    track_docs = {}\n",
    "    index = 0\n",
    "    while find_str(html, 'tracknumber_value\">', index) != -1:\n",
    "\n",
    "        index = find_str(html, 'tracknumber_value\">', index)\n",
    "        #print(index)\n",
    "        track_chunk = html[index:].split('<br>')[0]\n",
    "        #print(track_chunk)\n",
    "        \n",
    "        # Extract track number\n",
    "        track_num = track_chunk[:22].split('<')[0].strip('tracknumber_value\">')\n",
    "        #print('Track Number:', track_num)\n",
    "\n",
    "        # Extract track information\n",
    "        chunk_index = 0\n",
    "        chunk_index = find_str(track_chunk, 'meta itemprop=\"name\" content=', chunk_index)\n",
    "        extracted_value = track_chunk[chunk_index:].strip('meta itemprop=\"name\" content=').split('>')[0].strip('\"')\n",
    "        clean_string = fix_decoding_errors(extracted_value)\n",
    "        #print(clean_string)\n",
    "        \n",
    "        if len(clean_string) > 1:\n",
    "            try:\n",
    "                artist_list, track, remixer = parse_track_and_artist(clean_string)\n",
    "            except:\n",
    "                artist_list, track, remixer = None, None, None\n",
    "        else:\n",
    "            artist_list, track, remixer = None, None, None\n",
    "            \n",
    "        # Avoid ID's for now\n",
    "        if artist_list is None:\n",
    "            pass\n",
    "        # If track info pull failed then pass\n",
    "        elif (('ID' in artist_list) or ('ID' in track)): \n",
    "            pass\n",
    "        else:\n",
    "            \n",
    "            # Tends to be multiple artists so artists parsed to list even if only one\n",
    "            for artist in artist_list:\n",
    "                \n",
    "                #print('Artist:',artist)\n",
    "                #print('Track:', track)\n",
    "                #print('Remixer:', remixer)\n",
    "\n",
    "                # Extract artist page\n",
    "                artist_index = 0\n",
    "                artist_index = find_str(track_chunk, 'title=\"open artist page\"', artist_index)\n",
    "                if artist_index != -1:\n",
    "                    artist_url = track_chunk[artist_index:].split('class')[1].split('href=\"')[1].rstrip('\" ')\n",
    "                    #print('Aritst url:', artist_url)\n",
    "                    artist_url = 'https://www.1001tracklists.com' + artist_url\n",
    "                else:\n",
    "                    artist_url = 'N/A'\n",
    "\n",
    "                # Extract remixer page (if exists)\n",
    "                if remixer != 'N/A':\n",
    "                    remixer_index = find_str(track_chunk, 'title=\"open remixer artist page\"', artist_index)\n",
    "                    if remixer_index != -1:\n",
    "                        remixer_url = track_chunk[remixer_index:].split('class')[1].split('href=\"')[1].rstrip('\" ')\n",
    "                        #print('Remixer url:', remixer_url)\n",
    "                        remixer_url = 'https://www.1001tracklists.com' + remixer_url\n",
    "                    else:\n",
    "                        remixer_url = 'N/A'\n",
    "                else:\n",
    "                    remixer_url = 'N/A'\n",
    "\n",
    "                # Extract track page\n",
    "                track_index = 0\n",
    "                track_index = find_str(track_chunk, 'title=\"open track page\"', artist_index)\n",
    "                if track_index != -1:\n",
    "                    track_url = track_chunk[track_index:].split('class')[1].split('href=\"')[1].split('\"')[0]\n",
    "                    #print('track url:', track_url)\n",
    "                    track_url = 'https://www.1001tracklists.com' + track_url\n",
    "                else:\n",
    "                    track_url = 'N/A'\n",
    "\n",
    "                track_doc = {\\\n",
    "                            'track_num': track_num,\n",
    "                            'artist': artist.strip(' '),\n",
    "                            'artist_url': artist_url.strip(' '),\n",
    "                            'name': track.strip(' '),\n",
    "                            'track_url': track_url.strip(' '),\n",
    "                            'remixer': remixer.strip(' '),\n",
    "                            'remixer_url': remixer_url.strip(' ')\n",
    "                            }\n",
    "                track_docs[track_num] = track_doc\n",
    "                #print('\\n\\n\\n')\n",
    "\n",
    "    return track_docs\n",
    "\n",
    "def build_artist_edges(url_doc, url):\n",
    "    '''\n",
    "    Build artist set-adjacency docs -- order n^2.\n",
    "    Dont iterate over full set twice since will be considered non-directional\n",
    "    '''\n",
    "    all_tracks = []\n",
    "    these_tracks = list(url_doc['track_docs'].values())\n",
    "    for i in range(len(these_tracks)):\n",
    "        for j in range(i,len(these_tracks)):\n",
    "\n",
    "            track = these_tracks[i]\n",
    "            other_track = these_tracks[j]\n",
    "\n",
    "            first_artist = track['artist']\n",
    "            second_artist = other_track['artist']\n",
    "\n",
    "            if first_artist != second_artist:\n",
    "                all_tracks.append(\\\n",
    "                                {\n",
    "                                'artist1': first_artist,\n",
    "                                'artist2': second_artist,\n",
    "                                'url': url\n",
    "                                }\n",
    "                            )\n",
    "    return all_tracks\n",
    "\n",
    "def build_track_edges(track_docs, url):\n",
    "    '''\n",
    "    Build track set-adjacency docs -- order n^2.\n",
    "    Dont iterate over full set twice since will be considered non-directional\n",
    "    '''\n",
    "    edge_docs = {}\n",
    "    keys = sorted(list(track_docs.keys()))\n",
    "    for i in range(len(keys)):\n",
    "        for j in range(i, len(keys)):\n",
    "\n",
    "            key = keys[i]\n",
    "            other_key = keys[j]\n",
    "\n",
    "            if key != other_key:\n",
    "                _id = '_'.join([url,'_'.join(key),'_'.join(other_key)])\n",
    "                edge_docs[_id] = \\\n",
    "                                    {\n",
    "                                    #'_id': _id,\n",
    "                                    'track1_name': track_docs[key]['name'],\n",
    "                                    'track1_artist': track_docs[key]['artist'],\n",
    "                                    'track1_remixer': track_docs[key]['remixer'],\n",
    "                                    'track2_name': track_docs[other_key]['name'],\n",
    "                                    'track2_artist': track_docs[key]['artist'],\n",
    "                                    'track2_remixer': track_docs[key]['remixer'],\n",
    "                                    'url': url\n",
    "                                    }\n",
    "    return edge_docs\n",
    "                \n",
    "def build_sequential_track_edges(track_docs, url):\n",
    "    '''\n",
    "    Allows for later \"next track lookup\" functionality\n",
    "\n",
    "    '''\n",
    "    enumerated_tracks = [(track_docs[key]['track_num'], track_docs[key])\\\n",
    "                             for key in list(track_docs.keys())]\n",
    "    enumerated_tracks = sorted(enumerated_tracks, key=lambda x: x[0])\n",
    "    \n",
    "    seq_docs = {}\n",
    "    for track_idx in range(len(enumerated_tracks)-1):\n",
    "        _id = '_'.join(\\\n",
    "                      [\\\n",
    "                       url,\\\n",
    "                       '_'.join(enumerated_tracks[track_idx][0]),\\\n",
    "                       '_'.join(enumerated_tracks[track_idx+1][0])\n",
    "                      ]\n",
    "                    )\n",
    "        seq_docs[_id] = \\\n",
    "                       {\n",
    "                       #'_id': _id,\n",
    "                       'url': url,\n",
    "                       'track1_name': enumerated_tracks[track_idx][1]['name'],\n",
    "                       'track1_artist': enumerated_tracks[track_idx][1]['artist'],\n",
    "                       'track1_remixer': enumerated_tracks[track_idx][1]['remixer'],\n",
    "                       'track2_name': enumerated_tracks[track_idx+1][1]['name'],\n",
    "                       'track2_artist': enumerated_tracks[track_idx+1][1]['artist'],\n",
    "                       'track2_remixer': enumerated_tracks[track_idx+1][1]['remixer'],\n",
    "                       'first_position': str(enumerated_tracks[track_idx][0]),\n",
    "                       'second_position': str(enumerated_tracks[track_idx+1][0]),   \n",
    "                       }\n",
    "    return seq_docs\n",
    "\n",
    "def build_played_playedby_edge(url_doc, url):\n",
    "    '''\n",
    "    Allows you to map who plays who.\n",
    "    I think it would be interesting to study directional graphs from this.\n",
    "    \n",
    "    '''\n",
    "    dj_name = url_doc['dj_name']\n",
    "    dj_url = url_doc['dj_url']\n",
    "    \n",
    "    if (dj_name == 'N/A') or (dj_url == 'N/A'):\n",
    "        return []\n",
    "    \n",
    "    played_docs = []\n",
    "    for track_doc in list(url_doc['track_docs'].values()):\n",
    "        \n",
    "        played_docs.append(\\\n",
    "                          {\n",
    "                          'url': url,\n",
    "                          'played_by': dj_name,\n",
    "                          'played_by_url': dj_url,\n",
    "                          'played': track_doc['name'],\n",
    "                          'played_track_url': track_doc['track_url'],\n",
    "                          'played_artist': track_doc['artist'],\n",
    "                          'played_artist_url': track_doc['artist_url'],\n",
    "                          'played_remixer': track_doc['remixer'],\n",
    "                          'played_remixer_url': track_doc['remixer_url']\n",
    "                          }\n",
    "                        )\n",
    "    return played_docs\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spot check parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of track connections: 91\n",
      "Length of Sequential connections: 13\n",
      "Length of played connections: 14\n",
      "Length of artist connections: 85\n"
     ]
    }
   ],
   "source": [
    "# from urllib.request import Request, urlopen\n",
    "\n",
    "# url = 'https://www.1001tracklists.com/tracklist/2rrcqmpk/modd-vulcan-gas-company-austin-united-states-2019-03-09.html'\n",
    "# req = Request(url,\\\n",
    "#               headers={'User-Agent': 'Mozilla/5.0'})\n",
    "# html = str(urlopen(req).read())\n",
    "\n",
    "# url_doc = {}\n",
    "# url_doc['html'] = html\n",
    "# url_doc.update(tracklist_meta_data(html))\n",
    "# url_doc.update(tracklist_general_information(html))\n",
    "# track_docs = tracklist_track_data(html)\n",
    "# url_doc['track_docs'] = track_docs\n",
    "\n",
    "# track_edges = build_track_edges(track_docs, url)\n",
    "# print('Length of track connections:', len(track_edges))\n",
    "# sequential_edges = build_sequential_track_edges(track_docs, url)\n",
    "# print('Length of Sequential connections:', len(sequential_edges))\n",
    "# played_edges = build_played_playedby_edge(url_doc, url)\n",
    "# print('Length of played connections:', len(played_edges))\n",
    "# artist_edges = build_artist_edges(url_doc, url)\n",
    "# print('Length of artist connections:', len(artist_edges))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run over these and keep a depth of like 2 for POC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate over newest setlists and grap urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# def find_str(s, char, start_index=0):\n",
    "\n",
    "#     index = 0\n",
    "#     s = s[start_index+1:]\n",
    "#     if char in s:\n",
    "#         c = char[0]\n",
    "#         for ch in s:\n",
    "#             if ch == c:\n",
    "#                 if s[index:index+len(char)] == char:\n",
    "#                     return start_index + 1 + index\n",
    "#             index += 1\n",
    "#     return -1   \n",
    "\n",
    "# def request(url):\n",
    "\n",
    "#     user_agent = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'\n",
    "#     req = Request(url,\\\n",
    "#                   headers={'User-Agent': user_agent})\n",
    "#     html = str(urlopen(req).read())\n",
    "#     return html\n",
    "\n",
    "# date_pages = [\"https://www.1001tracklists.com/index%d.html?order=date\" %d for d in range(100)]\n",
    "# urls = []\n",
    "# for url in date_pages:\n",
    "    \n",
    "#     # Make http request\n",
    "#     html = request(url)\n",
    "#     index = 0\n",
    "#     # Iterate over links found in html\n",
    "#     count = 0\n",
    "#     while find_str(html, 'a href=\"', index) != -1:\n",
    "        \n",
    "#         # Extract url\n",
    "#         index = find_str(html, 'a href=\"', index)\n",
    "#         url_chunk = html[index:].split('\"')[1]\n",
    "\n",
    "#         # Make sure it is either a referenced tracklist or 1001 page\n",
    "#         if ('/tracklist/' in url_chunk) and ('http' not in url_chunk):\n",
    "#             new_page = 'https://www.1001tracklists.com' + url_chunk\n",
    "#             urls.append(new_page)\n",
    "#         if ('www.1001tracklists.com' in url_chunk) and ('.html' in url_chunk):\n",
    "#             urls.append(url_chunk)\n",
    "            \n",
    "#         count += 1\n",
    "            \n",
    "#     print('Number of URLS:', len(urls))\n",
    "#     time.sleep(10)\n",
    "\n",
    "# import pickle\n",
    "# with open('1001_urls.pkl', 'wb') as f:\n",
    "#      pickle.dump(urls, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4534\n",
      "2980\n",
      "2970\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np \n",
    "# import pickle\n",
    "\n",
    "# with open('1001_urls.pkl', 'rb') as f:\n",
    "#      urls = pickle.load(f)\n",
    "        \n",
    "# print(len(urls))\n",
    "# unique_urls = np.unique(urls)\n",
    "# print(len(unique_urls))\n",
    "# unique_urls = [url for url in unique_urls if ('.html' in url) and ('#tlp' not in url)]\n",
    "# print(len(unique_urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate over found urls and parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "# import pickle\n",
    "# import numpy as np\n",
    "\n",
    "# def parse(url):\n",
    "    \n",
    "#     req = Request(url,\\\n",
    "#                   headers={'User-Agent': 'Mozilla/5.0'})\n",
    "#     html = str(urlopen(req).read())   \n",
    "    \n",
    "#     url_doc = {}\n",
    "#     url_doc['url'] = url\n",
    "#     url_doc['html'] = html\n",
    "#     url_doc.update(tracklist_meta_data(html))\n",
    "#     url_doc.update(tracklist_general_information(html))\n",
    "\n",
    "#     track_docs = tracklist_track_data(html)\n",
    "#     url_doc['track_docs'] = track_docs\n",
    "\n",
    "#     track_edges = build_track_edges(track_docs, url)\n",
    "#     print('Length of track connections:', len(track_edges))\n",
    "#     sequential_edges = build_sequential_track_edges(track_docs, url)\n",
    "#     print('Length of Sequential connections:', len(sequential_edges))\n",
    "#     played_edges = build_played_playedby_edge(url_doc, url)\n",
    "#     print('Length of played connections:', len(played_edges))\n",
    "#     artist_edges = build_artist_edges(url_doc, url)\n",
    "#     print('Length of artist connections:', len(artist_edges))\n",
    "\n",
    "#     return url_doc, track_edges, sequential_edges, played_edges, artist_edges, html\n",
    "\n",
    "    \n",
    "# played_docs = []\n",
    "# sequential_docs = []\n",
    "# track_docs = []\n",
    "# tracklist_docs = []\n",
    "# artist_docs = []\n",
    "# url_html_map = {}\n",
    "\n",
    "# with open('1001_urls.pkl', 'rb') as f:\n",
    "#     urls = pickle.load(f)\n",
    "\n",
    "# # Find unique urls\n",
    "# seen_urls = []\n",
    "# unique_urls = np.unique(urls)\n",
    "# unique_urls = [url for url in unique_urls\\\n",
    "#                    if ('.html' in url) and ('#tlp' not in url) and (url not in seen_urls)]\n",
    "\n",
    "# for url in unique_urls[:]:\n",
    "    \n",
    "#     if ('.html' in url) and ('#tlp' not in url) and (url not in seen_urls):\n",
    "    \n",
    "#         try:\n",
    "            \n",
    "#             url_doc, track_edges, sequential_edges, played_edges, artist_edges, html = parse(url)\n",
    "\n",
    "#             url_html_map[url] = html\n",
    "            \n",
    "#             played_docs.extend(played_edges)\n",
    "#             sequential_docs.extend(sequential_edges)\n",
    "#             track_docs.extend(track_edges)\n",
    "#             tracklist_docs.append(url_doc)\n",
    "#             artist_docs.extend(artist_edges)\n",
    "\n",
    "#             print('Len played docs:', len(played_docs))\n",
    "#             print('Len sequential docs:', len(sequential_docs))\n",
    "#             print('Len track docs:', len(track_docs))\n",
    "#             print('Len tracklist docs:', len(tracklist_docs))\n",
    "#             print('Len artist docs:', len(artist_docs))\n",
    "            \n",
    "#             with open('played_docs3.pkl', 'wb') as f:\n",
    "#                 pickle.dump(played_docs, f)\n",
    "#             with open('sequential_docs3.pkl', 'wb') as f:\n",
    "#                 pickle.dump(sequential_docs, f)\n",
    "#             with open('track_docs3.pkl', 'wb') as f:\n",
    "#                 pickle.dump(track_docs, f)\n",
    "#             with open('tracklist_docs3.pkl', 'wb') as f:\n",
    "#                 pickle.dump(tracklist_docs, f)\n",
    "#             with open('artist_docs3.pkl', 'wb') as f:\n",
    "#                 pickle.dump(artist_docs, f)\n",
    "#             with open('url_html_map.pkl', 'wb') as f:\n",
    "#                 pickle.dump(artist_docs, f)\n",
    "                \n",
    "#             seen_urls.append(url)\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "        \n",
    "#         print('waiting')\n",
    "#         time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix sequential and track docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ftfy\n",
    "# import pymongo\n",
    "# from urllib.request import Request, urlopen\n",
    "\n",
    "# class Parser:\n",
    "    \n",
    "#     def __init__(self):\n",
    "        \n",
    "#         myclient = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "#         db = myclient['1001']\n",
    "        \n",
    "#         self.url_html_map = db['url_html_map']\n",
    "#         self.tracklist_collection = db['tracklist_docs']\n",
    "#         self.played_collection = db['played_docs']\n",
    "#         self.track_collection = db['track_docs_fixed']\n",
    "#         self.artist_collection = db['artist_docs']\n",
    "#         self.sequential_collection = db['sequential_docs_fixed']\n",
    "        \n",
    "#         self.track_docs = []\n",
    "#         self.played_docs = []\n",
    "#         self.artist_docs = []\n",
    "#         self.tracklist_docs = []\n",
    "#         self.sequential_docs = []\n",
    "\n",
    "#     def request(self, url):\n",
    "\n",
    "#         req = Request(url,\\\n",
    "#                       headers={'User-Agent': 'Mozilla/5.0'})\n",
    "#         html = str(urlopen(req).read())\n",
    "#         return html\n",
    "        \n",
    "#     def find_str(self, s, char, start_index=0):\n",
    "#         '''\n",
    "#         Find substring char in string s. Found on internet, probably not efficient.\n",
    "\n",
    "#         '''\n",
    "#         index = 0\n",
    "#         s = s[start_index+1:]\n",
    "#         if char in s:\n",
    "#             c = char[0]\n",
    "#             for ch in s:\n",
    "#                 if ch == c:\n",
    "#                     if s[index:index+len(char)] == char:\n",
    "#                         return start_index + 1 + index\n",
    "#                 index += 1\n",
    "#         return -1\n",
    "\n",
    "#     def fix_decoding_errors(self, string):\n",
    "#         '''\n",
    "#         Fix UTF-8 decoding issues. Probably need to find more systematic/thorough approach to this.\n",
    "\n",
    "#         REPLACE THIS WITH ftfy.fix_text() -- python package which should be one stop shop for fixes\n",
    "#         '''\n",
    "#         string = string.replace('&amp;','&')\n",
    "#         string = string.replace('&#39;',\"'\")\n",
    "#         string = string.replace('\\\\xc3\\\\xb6','o')\n",
    "#         string = string.replace('\\\\xc3\\\\xab','e')\n",
    "#         string = string.replace('\\\\xc3\\\\x9','u')\n",
    "#         string = string.replace('\\\\xc3\\\\xb8','o')\n",
    "#         string = string.replace(\"\\\\'\",\"'\")\n",
    "\n",
    "#         return ftfy.fix_text(string)\n",
    "\n",
    "#     def parse_track_and_artist(self, track_string):\n",
    "#         '''\n",
    "#         Extract the artist, track name, and remixer (if any) from the standard formatting used by 1001.\n",
    "\n",
    "#         '''\n",
    "#         # Check if Remix/Bootleg/Edit and parse accordingly\n",
    "#         if ('Remix' in track_string) or ('Bootleg' in track_string) or ('Edit' in track_string):\n",
    "\n",
    "#             artist, track_remixer = [string.strip(' ') for string in track_string.split(' - ')]\n",
    "#             track_remixer = [string.strip(' ') for string in track_remixer.split('(')]\n",
    "\n",
    "#             if len(track_remixer) > 2:\n",
    "#                 track = track_remixer[0]\n",
    "#                 remixer = '('.join(track_remixer[1:])\n",
    "#             else:\n",
    "#                 track, remixer = track_remixer\n",
    "#                 remixer = remixer.rstrip('Remix)').strip(' ')\n",
    "\n",
    "#         # If not remix, then should follow standard layout \"Artist Name - Track Name\"\n",
    "#         # This layout is expressed explicitly in html\n",
    "#         else:\n",
    "\n",
    "#             artist, track = [string.strip(' ') for string in track_string.split(' - ')]\n",
    "#             remixer = 'N/A'\n",
    "\n",
    "#         # Check for multiple artists -- Big Room sets tend to have hella mashups\n",
    "#         # Sometimes there is more structured formatting to exploit i.e. (Artist1 vs. Artist2 - Track1 vs. Track2)\n",
    "#         # Not worrying about that now b/c big room sux\n",
    "#         if 'vs.' in artist:\n",
    "#             artist = artist.replace('vs.','&')\n",
    "#         if '&' in artist:\n",
    "#             artist = [a.strip(' ') for a in artist.split('&')]\n",
    "\n",
    "#         # Remove features\n",
    "#         # We could make features a separate field but for now just removing\n",
    "#         if isinstance(artist, str):\n",
    "#             if ('feat.' in artist) or ('ft.' in artist):\n",
    "#                 artist = artist.split('feat.')[0].strip(' ')\n",
    "#                 artist = artist.split('ft.')[0].strip(' ')\n",
    "#         if isinstance(artist, list):\n",
    "#                 artist = [a.split('feat.')[0].split('ft.')[0].strip(' ') for a in artist]\n",
    "\n",
    "#         if isinstance(artist, list):\n",
    "#             return (artist, track, remixer)\n",
    "#         else:\n",
    "#             return ([artist], track, remixer)\n",
    "\n",
    "\n",
    "#     def tracklist_meta_data(self, html):\n",
    "#         '''\n",
    "#         Extract meta data about tracklist/set.\n",
    "\n",
    "#         '''\n",
    "#         meta_data = {}\n",
    "\n",
    "#         # Extract set description\n",
    "#         index = 0\n",
    "#         start_term = 'meta name=\"description\" content=\"'\n",
    "#         index = self.find_str(html, start_term, index)\n",
    "#         description = html[index:].split('>')[0]\n",
    "#         description = description.lstrip(start_term).rstrip('\"')\n",
    "#         meta_data['description'] = description\n",
    "\n",
    "#         # Set creation date\n",
    "#         # Should probably be the point in time we use for building prediction data\n",
    "#         index = 0\n",
    "#         start_term = 'meta name=\"dcterms.created\" content=\"'\n",
    "#         index = self.find_str(html, start_term, index)\n",
    "#         created = html[index:].split('>')[0]\n",
    "#         created = created.lstrip(start_term).rstrip('\"')\n",
    "#         meta_data['created'] = created\n",
    "\n",
    "#         # Set last modified data\n",
    "#         index = 0\n",
    "#         start_term = 'meta name=\"dcterms.modified\" content=\"'\n",
    "#         index = self.find_str(html, start_term, index)\n",
    "#         modified = html[index:].split('>')[0]\n",
    "#         modified = modified.lstrip(start_term).rstrip('\"')\n",
    "#         meta_data['modified'] = modified\n",
    "\n",
    "#         return meta_data\n",
    "\n",
    "#     def tracklist_general_information(self, html):\n",
    "#         '''\n",
    "#         Extract general info about tracklist/set.\n",
    "\n",
    "#         '''\n",
    "#         info_doc = {}\n",
    "#         index = 0\n",
    "#         start_term = 'General Information'\n",
    "#         index = self.find_str(html, start_term, index)\n",
    "#         info_chunk = html[index:].split('Most Liked Tracklists')[0]\n",
    "\n",
    "#         # Genres -- can use these to build genre-specific graphs\n",
    "#         style_index = 0\n",
    "#         style_index = self.find_str(info_chunk, 'Tracklist Musicstyle', style_index)\n",
    "#         styles = info_chunk[style_index:].split('id=\"tl_music_styles\">')[1].split('</td>')[0]\n",
    "#         styles = [style.strip(' ') for style in styles.split(',')]\n",
    "#         info_doc['styles'] = styles\n",
    "\n",
    "#         # If 1001 recognizes the dj who played the set they link their dj page\n",
    "#         # Its my understanding dj pages are independent of artist pages -- we'll need to map these\n",
    "#         index = 0\n",
    "#         start_term = 'a href=\"/dj'\n",
    "#         index = self.find_str(html, start_term, index)\n",
    "#         if index != -1:\n",
    "#             dj_url = html[index:].split('class')[0].split('\"')[1]\n",
    "#             dj_url = 'https://www.1001tracklists.com' + dj_url\n",
    "#             info_doc['dj_url'] = dj_url\n",
    "\n",
    "#             dj_name = html[index:].split('</a>')[0].split('>')[1]\n",
    "#             info_doc['dj_name'] = dj_name\n",
    "#         else:\n",
    "#             info_doc['dj_url'] = 'N/A'\n",
    "#             info_doc['dj_name'] = 'N/A'\n",
    "\n",
    "#         return info_doc\n",
    "        \n",
    "#     def tracklist_track_data(self, html):\n",
    "\n",
    "#         '''\n",
    "#         Extract track related data from set\n",
    "#         '''\n",
    "#         track_docs = {}\n",
    "#         index = 0\n",
    "#         while self.find_str(html, 'tracknumber_value\">', index) != -1:\n",
    "\n",
    "#             index = self.find_str(html, 'tracknumber_value\">', index)\n",
    "#             #print(index)\n",
    "#             track_chunk = html[index:].split('<br>')[0]\n",
    "\n",
    "#             # Extract track number\n",
    "#             track_num = track_chunk[:22].split('<')[0].strip('tracknumber_value\">')\n",
    "#             #print('Track Number:', track_num)\n",
    "\n",
    "#             # Extract track information\n",
    "#             chunk_index = 0\n",
    "#             chunk_index = self.find_str(track_chunk, 'meta itemprop=\"name\" content=', chunk_index)\n",
    "#             extracted_value = track_chunk[chunk_index:].strip('meta itemprop=\"name\" content=').split('>')[0].strip('\"')\n",
    "#             clean_string = self.fix_decoding_errors(extracted_value)\n",
    "#             #print(clean_string)\n",
    "\n",
    "#             if len(clean_string) > 1:\n",
    "#                 try:\n",
    "#                     artist_list, track, remixer = self.parse_track_and_artist(clean_string)\n",
    "#                 except:\n",
    "#                     artist_list, track, remixer = None, None, None\n",
    "#             else:\n",
    "#                 artist_list, track, remixer = None, None, None\n",
    "#             #print(artist_list, track, remixer)\n",
    "                \n",
    "#             # Avoid ID's for now\n",
    "#             if artist_list is None:\n",
    "#                 pass\n",
    "#             # If track info pull failed then pass\n",
    "#             elif (('ID' in artist_list) or ('ID' in track)):\n",
    "#                 pass\n",
    "#             else:\n",
    "\n",
    "#                 # Tends to be multiple artists so artists parsed to list even if only one\n",
    "#                 for artist in artist_list:\n",
    "\n",
    "#                     #print('Artist:',artist)\n",
    "#                     #print('Track:', track)\n",
    "#                     #print('Remixer:', remixer)\n",
    "\n",
    "#                     # Extract artist page\n",
    "#                     artist_index = 0\n",
    "#                     artist_index = self.find_str(track_chunk, 'title=\"open artist page\"', artist_index)\n",
    "#                     if artist_index != -1:\n",
    "#                         try:\n",
    "#                             artist_url = track_chunk[artist_index:].split('class')[1].split('href=\"')[1].rstrip('\" ')\n",
    "#                             artist_url = 'https://www.1001tracklists.com' + artist_url\n",
    "#                             #print('Aritst url:', artist_url)\n",
    "#                         except:\n",
    "#                             artist_url = 'N/A'\n",
    "#                     else:\n",
    "#                         artist_url = 'N/A'\n",
    "\n",
    "#                     # Extract remixer page (if exists)\n",
    "#                     if remixer != 'N/A':\n",
    "#                         remixer_index = self.find_str(track_chunk, 'title=\"open remixer artist page\"', artist_index)\n",
    "#                         if remixer_index != -1:\n",
    "#                             try:\n",
    "#                                 remixer_url = track_chunk[remixer_index:].split('class')[1].split('href=\"')[1].rstrip('\" ')\n",
    "#                                 remixer_url = 'https://www.1001tracklists.com' + remixer_url\n",
    "#                                 #print('Remixer url:', remixer_url)\n",
    "#                             except:\n",
    "#                                 remixer_url = 'N/A'\n",
    "#                         else:\n",
    "#                             remixer_url = 'N/A'\n",
    "#                     else:\n",
    "#                         remixer_url = 'N/A'\n",
    "                    \n",
    "#                     # Extract track page\n",
    "#                     track_index = 0\n",
    "#                     track_index = self.find_str(track_chunk, 'title=\"open track page\"', artist_index)\n",
    "#                     if track_index != -1:\n",
    "#                         try:\n",
    "#                             track_url = track_chunk[track_index:].split('class')[1].split('href=\"')[1].split('\"')[0]\n",
    "#                             track_url = 'https://www.1001tracklists.com' + track_url\n",
    "#                             #print('track url:', track_url)\n",
    "#                         except:\n",
    "#                             track_url = 'N/A'\n",
    "#                     else:\n",
    "#                         track_url = 'N/A'\n",
    "\n",
    "#                     track_doc = {\\\n",
    "#                                 'track_num': track_num,\n",
    "#                                 'artist': artist.strip(' '),\n",
    "#                                 'artist_url': artist_url.strip(' '),\n",
    "#                                 'name': track.strip(' '),\n",
    "#                                 'track_url': track_url.strip(' '),\n",
    "#                                 'remixer': remixer.strip(' '),\n",
    "#                                 'remixer_url': remixer_url.strip(' ')\n",
    "#                                 }\n",
    "#                     track_docs[track_num] = track_doc\n",
    "#                     #print('\\n\\n\\n')\n",
    "        \n",
    "#         #print(len(track_docs.keys()))\n",
    "#         return track_docs\n",
    "\n",
    "#     def build_artist_edges(self, url_doc, url):\n",
    "#         '''\n",
    "#         Build artist set-adjacency docs -- order n^2.\n",
    "#         Dont iterate over full set twice since will be considered non-directional\n",
    "#         '''\n",
    "#         all_tracks = {}\n",
    "#         count = 0\n",
    "#         these_tracks = list(url_doc['track_docs'].values())\n",
    "#         for i in range(len(these_tracks)):\n",
    "#             for j in range(i,len(these_tracks)):\n",
    "\n",
    "#                 track = these_tracks[i]\n",
    "#                 other_track = these_tracks[j]\n",
    "\n",
    "#                 first_artist = track['artist']\n",
    "#                 second_artist = other_track['artist']\n",
    "\n",
    "#                 if first_artist != second_artist:\n",
    "#                     _id = '_'.join([url,first_artist,second_artist])\n",
    "#                     all_tracks[_id] = \\\n",
    "#                                         {\n",
    "#                                         #'_id': _id,\n",
    "#                                         'artist1': first_artist,\n",
    "#                                         'artist2': second_artist,\n",
    "#                                         'url': url\n",
    "#                                         }\n",
    "#         return all_tracks\n",
    "\n",
    "#     def build_track_edges(self, track_docs, url):\n",
    "#         '''\n",
    "#         Build track set-adjacency docs -- order n^2.\n",
    "#         Dont iterate over full set twice since will be considered non-directional\n",
    "#         '''\n",
    "#         edge_docs = {}\n",
    "#         keys = sorted(list(track_docs.keys()))\n",
    "#         for i in range(len(keys)):\n",
    "#             for j in range(i, len(keys)):\n",
    "\n",
    "#                 key = keys[i]\n",
    "#                 other_key = keys[j]\n",
    "\n",
    "#                 if key != other_key:\n",
    "#                     _id = '_'.join([url,'_'.join(key),'_'.join(other_key)])\n",
    "#                     edge_docs[_id] = \\\n",
    "#                                         {\n",
    "#                                         #'_id': _id,\n",
    "#                                         'track1_name': track_docs[key]['name'],\n",
    "#                                         'track1_artist': track_docs[key]['artist'],\n",
    "#                                         'track1_remixer': track_docs[key]['remixer'],\n",
    "#                                         'track2_name': track_docs[other_key]['name'],\n",
    "#                                         'track2_artist': track_docs[key]['artist'],\n",
    "#                                         'track2_remixer': track_docs[key]['remixer'],\n",
    "#                                         'url': url\n",
    "#                                         }\n",
    "#         return edge_docs\n",
    "                \n",
    "#     def build_sequential_track_edges(self, track_docs, url):\n",
    "#         '''\n",
    "#         Allows for later \"next track lookup\" functionality\n",
    "\n",
    "#         '''\n",
    "#         enumerated_tracks = [(track_docs[key]['track_num'], track_docs[key])\\\n",
    "#                                  for key in list(track_docs.keys())]\n",
    "#         enumerated_tracks = sorted(enumerated_tracks, key=lambda x: x[0])\n",
    "\n",
    "#         seq_docs = {}\n",
    "#         for track_idx in range(len(enumerated_tracks)-1):\n",
    "#             _id = '_'.join(\\\n",
    "#                           [\\\n",
    "#                            url,\\\n",
    "#                            '_'.join(enumerated_tracks[track_idx][0]),\\\n",
    "#                            '_'.join(enumerated_tracks[track_idx+1][0])\n",
    "#                           ]\n",
    "#                         )\n",
    "#             seq_docs[_id] = \\\n",
    "#                            {\n",
    "#                            #'_id': _id,\n",
    "#                            'url': url,\n",
    "#                            'track1_name': enumerated_tracks[track_idx][1]['name'],\n",
    "#                            'track1_artist': enumerated_tracks[track_idx][1]['artist'],\n",
    "#                            'track1_remixer': enumerated_tracks[track_idx][1]['remixer'],\n",
    "#                            'track2_name': enumerated_tracks[track_idx+1][1]['name'],\n",
    "#                            'track2_artist': enumerated_tracks[track_idx+1][1]['artist'],\n",
    "#                            'track2_remixer': enumerated_tracks[track_idx+1][1]['remixer'],\n",
    "#                            'first_position': str(enumerated_tracks[track_idx][0]),\n",
    "#                            'second_position': str(enumerated_tracks[track_idx+1][0]),   \n",
    "#                            }\n",
    "#         return seq_docs\n",
    "\n",
    "#     def build_played_playedby_edge(self, url_doc, url):\n",
    "#         '''\n",
    "#         Allows you to map who plays who.\n",
    "#         I think it would be interesting to study directional graphs from this.\n",
    "\n",
    "#         '''\n",
    "#         dj_name = url_doc['dj_name']\n",
    "#         dj_url = url_doc['dj_url']\n",
    "\n",
    "#         if (dj_name == 'N/A') or (dj_url == 'N/A'):\n",
    "#             return {}\n",
    "\n",
    "#         played_docs = {}\n",
    "#         for track_doc in list(url_doc['track_docs'].values()):\n",
    "#             _id = '_'.join([url,dj_name,track_doc['name']])\n",
    "#             played_docs[_id] = \\\n",
    "#                               {\n",
    "#                               #'_id': _id,\n",
    "#                               'url': url,\n",
    "#                               'played_by': dj_name,\n",
    "#                               'played_by_url': dj_url,\n",
    "#                               'played': track_doc['name'],\n",
    "#                               'played_track_url': track_doc['track_url'],\n",
    "#                               'played_artist': track_doc['artist'],\n",
    "#                               'played_artist_url': track_doc['artist_url'],\n",
    "#                               'played_remixer': track_doc['remixer'],\n",
    "#                               'played_remixer_url': track_doc['remixer_url']\n",
    "#                               }\n",
    "#         return played_docs\n",
    "    \n",
    "    \n",
    "#     def parse(self, url, html): \n",
    "\n",
    "#         url_doc = {}\n",
    "#         url_doc['url'] = url\n",
    "#         url_doc['html'] = html\n",
    "#         self.url_html_map.insert_one(url_doc)\n",
    "            \n",
    "#         try:\n",
    "            \n",
    "#             url_doc.update(self.tracklist_meta_data(html))\n",
    "#             url_doc.update(self.tracklist_general_information(html))\n",
    "\n",
    "#             track_docs = self.tracklist_track_data(html)\n",
    "#             url_doc['track_docs'] = track_docs\n",
    "\n",
    "#             track_edges = self.build_track_edges(track_docs, url).values()\n",
    "#             sequential_edges = self.build_sequential_track_edges(track_docs, url).values()\n",
    "#             played_edges = self.build_played_playedby_edge(url_doc, url).values()\n",
    "#             artist_edges = self.build_artist_edges(url_doc, url).values()\n",
    "\n",
    "# #             self.tracklist_collection.insert_one(url_doc)\n",
    "#             for doc in track_edges:\n",
    "#                 self.track_collection.insert_one(doc)\n",
    "# #             for doc in artist_edges:\n",
    "# #                 self.artist_collection.insert_one(doc)\n",
    "# #             for doc in played_edges:\n",
    "# #                 self.played_collection.insert_one(doc)\n",
    "#             for doc in sequential_edges:\n",
    "#                 self.sequential_collection.insert_one(doc)\n",
    "\n",
    "#             self.tracklist_docs.append(url_doc)\n",
    "#             #self.played_docs.extend(played_edges)\n",
    "#             self.track_docs.extend(track_edges)\n",
    "#             #self.artist_docs.extend(artist_edges)        \n",
    "#             self.sequential_docs.extend(sequential_edges)\n",
    "\n",
    "#             print('Len tracklist docs:', len(self.tracklist_docs))\n",
    "#             #print('Len played docs:', len(self.played_docs))\n",
    "#             print('Len sequential docs:', len(self.sequential_docs))\n",
    "#             print('Len track docs:', len(self.track_docs))\n",
    "#             #print('Len artist docs:', len(self.artist_docs))\n",
    "        \n",
    "#         except Exception as e:\n",
    "#             print(str(e))\n",
    "\n",
    "# import pymongo\n",
    "\n",
    "# myclient = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "\n",
    "# mydb = myclient['1001']\n",
    "# url_docs = list(mydb['url_html_map'].find({}))\n",
    "# print(len(url_docs))\n",
    "\n",
    "# prsr = Parser()\n",
    "# for doc in url_docs[:]:\n",
    "#     print(doc['url'])\n",
    "#     if ('/tracklist/' in doc['url']) and ('http' in doc['url']) and ('#tlp' not in doc['url']):\n",
    "#         prsr.parse(doc['url'], doc['html'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
