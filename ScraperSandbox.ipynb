{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "\n",
    "myclient = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "\n",
    "mydb = myclient['1001']\n",
    "mydb['track_docs'].delete_many({})\n",
    "mydb['artist_docs'].delete_many({})\n",
    "mydb['played_docs'].delete_many({})\n",
    "mydb['tracklist_docs'].delete_many({})\n",
    "mydb['sequential_docs'].delete_many({})\n",
    "print(len(list(mydb['track_docs'].find({}))))\n",
    "print(len(list(mydb['artist_docs'].find({}))))\n",
    "print(len(list(mydb['played_docs'].find({}))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth: 50\n",
      "Depth: 50\n",
      "Depth: 50\n",
      "Depth: 50\n",
      "Depth: 50\n",
      "Depth: 50\n",
      "Depth: 50\n",
      "Depth: 50\n",
      "Depth: 50\n",
      "Depth: 49\n"
     ]
    }
   ],
   "source": [
    "def tracklist_track_data(html):\n",
    "    \n",
    "    '''\n",
    "    Extract track related data from set\n",
    "    '''\n",
    "    track_docs = {}\n",
    "    index = 0\n",
    "    while find_str(html, 'tracknumber_value\">', index) != -1:\n",
    "\n",
    "        index = find_str(html, 'tracknumber_value\">', index)\n",
    "        #print(index)\n",
    "        track_chunk = html[index:].split('<br>')[0]\n",
    "\n",
    "        # Extract track number\n",
    "        track_num = track_chunk[:22].split('<')[0].strip('tracknumber_value\">')\n",
    "        #print('Track Number:', track_num)\n",
    "\n",
    "        # Extract track information\n",
    "        chunk_index = 0\n",
    "        chunk_index = find_str(track_chunk, 'meta itemprop=\"name\" content=', chunk_index)\n",
    "        extracted_value = track_chunk[chunk_index:].strip('meta itemprop=\"name\" content=').split('>')[0].strip('\"')\n",
    "        clean_string = fix_decoding_errors(extracted_value)\n",
    "        #print(clean_string)\n",
    "        \n",
    "        if len(clean_string) > 1:\n",
    "            try:\n",
    "                artist_list, track, remixer = parse_track_and_artist(clean_string)\n",
    "            except:\n",
    "                artist_list, track, remixer = None, None, None\n",
    "        else:\n",
    "            artist_list, track, remixer = None, None, None\n",
    "            \n",
    "        # Avoid ID's for now\n",
    "        if artist_list is None:\n",
    "            pass\n",
    "        # If track info pull failed then pass\n",
    "        elif (('ID' in artist_list) or ('ID' in track)):\n",
    "            pass\n",
    "        else:\n",
    "            \n",
    "            # Tends to be multiple artists so artists parsed to list even if only one\n",
    "            for artist in artist_list:\n",
    "                \n",
    "                #print('Artist:',artist)\n",
    "                #print('Track:', track)\n",
    "                #print('Remixer:', remixer)\n",
    "\n",
    "                # Extract artist page\n",
    "                artist_index = 0\n",
    "                artist_index = find_str(track_chunk, 'title=\"open artist page\"', artist_index)\n",
    "                if artist_index != -1:\n",
    "                    try:\n",
    "                        artist_url = track_chunk[artist_index:].split('class')[1].split('href=\"')[1].rstrip('\" ')\n",
    "                        artist_url = 'https://www.1001tracklists.com' + artist_url\n",
    "                        #print('Aritst url:', artist_url)\n",
    "                    except:\n",
    "                        artist_url = 'N/A'\n",
    "                else:\n",
    "                    artist_url = 'N/A'\n",
    "\n",
    "                # Extract remixer page (if exists)\n",
    "                if remixer != 'N/A':\n",
    "                    remixer_index = find_str(track_chunk, 'title=\"open remixer artist page\"', artist_index)\n",
    "                    if remixer_index != -1:\n",
    "                        try:\n",
    "                            remixer_url = track_chunk[remixer_index:].split('class')[1].split('href=\"')[1].rstrip('\" ')\n",
    "                            remixer_url = 'https://www.1001tracklists.com' + remixer_url\n",
    "                            #print('Remixer url:', remixer_url)\n",
    "                        except:\n",
    "                            remixer_url = 'N/A'\n",
    "                    else:\n",
    "                        remixer_url = 'N/A'\n",
    "                else:\n",
    "                    remixer_url = 'N/A'\n",
    "\n",
    "                # Extract track page\n",
    "                track_index = 0\n",
    "                track_index = find_str(track_chunk, 'title=\"open track page\"', artist_index)\n",
    "                if track_index != -1:\n",
    "                    try:\n",
    "                        track_url = track_chunk[track_index:].split('class')[1].split('href=\"')[1].split('\"')[0]\n",
    "                        track_url = 'https://www.1001tracklists.com' + track_url\n",
    "                        #print('track url:', track_url)\n",
    "                    except:\n",
    "                        track_url = 'N/A'\n",
    "                else:\n",
    "                    track_url = 'N/A'\n",
    "\n",
    "                track_doc = {\\\n",
    "                            'track_num': track_num,\n",
    "                            'artist': artist.strip(' '),\n",
    "                            'artist_url': artist_url.strip(' '),\n",
    "                            'name': track.strip(' '),\n",
    "                            'track_url': track_url.strip(' '),\n",
    "                            'remixer': remixer.strip(' '),\n",
    "                            'remixer_url': remixer_url.strip(' ')\n",
    "                            }\n",
    "                track_docs[track_num] = track_doc\n",
    "                #print('\\n\\n\\n')\n",
    "\n",
    "    return track_docs\n",
    "\n",
    "def build_artist_edges(url_doc, url):\n",
    "    '''\n",
    "    Build artist set-adjacency docs -- order n^2.\n",
    "    Dont iterate over full set twice since will be considered non-directional\n",
    "    '''\n",
    "    all_tracks = {}\n",
    "    count = 0\n",
    "    these_tracks = list(url_doc['track_docs'].values())\n",
    "    for i in range(len(these_tracks)):\n",
    "        for j in range(i,len(these_tracks)):\n",
    "\n",
    "            track = these_tracks[i]\n",
    "            other_track = these_tracks[j]\n",
    "\n",
    "            first_artist = track['artist']\n",
    "            second_artist = other_track['artist']\n",
    "\n",
    "            if first_artist != second_artist:\n",
    "                _id = '_'.join([url,first_artist,second_artist])\n",
    "                all_tracks[_id] = \\\n",
    "                                    {\n",
    "                                    #'_id': _id,\n",
    "                                    'artist1': first_artist,\n",
    "                                    'artist2': second_artist,\n",
    "                                    'url': url\n",
    "                                    }\n",
    "    return all_tracks\n",
    "\n",
    "def build_track_edges(track_docs, url):\n",
    "    '''\n",
    "    Build track set-adjacency docs -- order n^2.\n",
    "    Dont iterate over full set twice since will be considered non-directional\n",
    "    '''\n",
    "    edge_docs = {}\n",
    "    keys = sorted(list(track_docs.keys()))\n",
    "    for i in range(len(keys)):\n",
    "        for j in range(i, len(keys)):\n",
    "            \n",
    "            key = keys[i]\n",
    "            other_key = keys[j]\n",
    "            \n",
    "            if key != other_key:\n",
    "                _id = '_'.join([url,'_'.join(key),'_'.join(other_key)])\n",
    "                edge_docs[_id] = \\\n",
    "                                    {\n",
    "                                    #'_id': _id,\n",
    "                                    'track1': key,\n",
    "                                    'track2': other_key,\n",
    "                                    'url': url\n",
    "                                    }\n",
    "    return edge_docs\n",
    "                \n",
    "def build_sequential_track_edges(track_docs, url):\n",
    "    '''\n",
    "    Allows for later \"next track lookup\" functionality\n",
    "    \n",
    "    '''\n",
    "    enumerated_tracks = [(track_docs[key]['track_num'], key) for key in list(track_docs.keys())]\n",
    "    enumerated_tracks = sorted(enumerated_tracks, key=lambda x: x[0])\n",
    "    \n",
    "    seq_docs = {}\n",
    "    for track_idx in range(len(enumerated_tracks)-1):\n",
    "        _id = '_'.join(\\\n",
    "                      [\\\n",
    "                       url,\\\n",
    "                       '_'.join(enumerated_tracks[track_idx][1]),\\\n",
    "                       '_'.join(enumerated_tracks[track_idx+1][1])\n",
    "                      ]\n",
    "                    )\n",
    "        seq_docs[_id] = \\\n",
    "                       {\n",
    "                       #'_id': _id,\n",
    "                       'url': url,\n",
    "                       'first_track': enumerated_tracks[track_idx][1],\n",
    "                       'second_track': enumerated_tracks[track_idx+1][1],\n",
    "                       'first_position': str(enumerated_tracks[track_idx][0]),\n",
    "                       'second_position': str(enumerated_tracks[track_idx+1][0]),   \n",
    "                       }\n",
    "    return seq_docs\n",
    "\n",
    "def build_played_playedby_edge(url_doc, url):\n",
    "    '''\n",
    "    Allows you to map who plays who.\n",
    "    I think it would be interesting to study directional graphs from this.\n",
    "    \n",
    "    '''\n",
    "    dj_name = url_doc['dj_name']\n",
    "    dj_url = url_doc['dj_url']\n",
    "    \n",
    "    if (dj_name == 'N/A') or (dj_url == 'N/A'):\n",
    "        return {}\n",
    "    \n",
    "    played_docs = {}\n",
    "    for track_doc in list(url_doc['track_docs'].values()):\n",
    "        _id = '_'.join([url,dj_name,track_doc['name']])\n",
    "        played_docs[_id] = \\\n",
    "                          {\n",
    "                          #'_id': _id,\n",
    "                          'url': url,\n",
    "                          'played_by': dj_name,\n",
    "                          'played_by_url': dj_url,\n",
    "                          'played': track_doc['name'],\n",
    "                          'played_track_url': track_doc['track_url'],\n",
    "                          'played_artist': track_doc['artist'],\n",
    "                          'played_artist_url': track_doc['artist_url'],\n",
    "                          'played_remixer': track_doc['remixer'],\n",
    "                          'played_remixer_url': track_doc['remixer_url']\n",
    "                          }\n",
    "    return played_docs\n",
    "\n",
    "\n",
    "\n",
    "class Crawler:\n",
    "    \n",
    "    def __init__(self, max_depth=1):\n",
    "        \n",
    "        import time\n",
    "        import pymongo\n",
    "        from urllib.request import Request, urlopen\n",
    "        \n",
    "        myclient = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "        db = myclient['1001']\n",
    "        \n",
    "        self.url_html_map = db['url_html_map']\n",
    "        self.tracklist_collection = db['tracklist_docs']\n",
    "        self.played_collection = db['played_docs']\n",
    "        self.track_collection = db['track_docs']\n",
    "        self.artist_collection = db['artist_docs']\n",
    "        self.sequential_collection = db['sequential_docs']\n",
    "        \n",
    "        self.stop_search = False\n",
    "        \n",
    "        self.urls = []\n",
    "        self.max_depth = max_depth\n",
    "        self.page_hash = {}\n",
    "        self.tracklist_hash = {}\n",
    "        \n",
    "        self.track_docs = []\n",
    "        self.played_docs = []\n",
    "        self.artist_docs = []\n",
    "        self.tracklist_docs = []\n",
    "        self.sequential_docs = []\n",
    "\n",
    "    def find_str(self, s, char, start_index=0):\n",
    "\n",
    "        index = 0\n",
    "        s = s[start_index+1:]\n",
    "        if char in s:\n",
    "            c = char[0]\n",
    "            for ch in s:\n",
    "                if ch == c:\n",
    "                    if s[index:index+len(char)] == char:\n",
    "                        return start_index + 1 + index\n",
    "                index += 1\n",
    "        return -1\n",
    "\n",
    "    def request(self,url):\n",
    "\n",
    "        req = Request(url,\\\n",
    "                      headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        html = str(urlopen(req).read())\n",
    "        return html\n",
    "    \n",
    "    def parse(self, url, html): \n",
    "\n",
    "        url_doc = {}\n",
    "        url_doc['url'] = url\n",
    "        url_doc['html'] = html\n",
    "        self.url_html_map.insert_one(url_doc)\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        GET RID OF THIS LATER\n",
    "        \n",
    "        '''\n",
    "        try:\n",
    "            \n",
    "            url_doc.update(tracklist_meta_data(html))\n",
    "            url_doc.update(tracklist_general_information(html))\n",
    "\n",
    "            track_docs = tracklist_track_data(html)\n",
    "            url_doc['track_docs'] = track_docs\n",
    "\n",
    "            track_edges = build_track_edges(track_docs, url).values()\n",
    "            sequential_edges = build_sequential_track_edges(track_docs, url).values()\n",
    "            played_edges = build_played_playedby_edge(url_doc, url).values()\n",
    "            artist_edges = build_artist_edges(url_doc, url).values()\n",
    "\n",
    "            self.tracklist_collection.insert_one(url_doc)\n",
    "            for doc in track_edges:\n",
    "                self.track_collection.insert_one(doc)\n",
    "            for doc in artist_edges:\n",
    "                self.artist_collection.insert_one(doc)\n",
    "            for doc in played_edges:\n",
    "                self.played_collection.insert_one(doc)\n",
    "            for doc in sequential_edges:\n",
    "                self.sequential_collection.insert_one(doc)\n",
    "\n",
    "            self.played_docs.extend(played_edges)\n",
    "            self.track_docs.extend(track_edges)\n",
    "            self.tracklist_docs.append(url_doc)\n",
    "            self.artist_docs.extend(artist_edges)        \n",
    "            self.sequential_docs.extend(sequential_edges)\n",
    "\n",
    "            print('Len played docs:', len(self.played_docs))\n",
    "            print('Len sequential docs:', len(self.sequential_docs))\n",
    "            print('Len track docs:', len(self.track_docs))\n",
    "            print('Len tracklist docs:', len(self.tracklist_docs))\n",
    "            print('Len artist docs:', len(self.artist_docs))\n",
    "\n",
    "            with open('played_docs_crawl.pkl', 'wb') as f:\n",
    "                pickle.dump(self.played_docs, f)\n",
    "            with open('sequential_docs_crawl.pkl', 'wb') as f:\n",
    "                pickle.dump(self.sequential_docs, f)\n",
    "            with open('track_docs_crawl.pkl', 'wb') as f:\n",
    "                pickle.dump(self.track_docs, f)\n",
    "            with open('tracklist_docs_crawl.pkl', 'wb') as f:\n",
    "                pickle.dump(self.tracklist_docs, f)\n",
    "            with open('artist_docs_crawl.pkl', 'wb') as f:\n",
    "                pickle.dump(self.artist_docs, f)\n",
    "            with open('url_html_map_crawl.pkl', 'wb') as f:\n",
    "                pickle.dump(self.page_hash, f)\n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    def crawl(self, url, depth):\n",
    "        \n",
    "        if len(self.tracklist_docs) == 5000:\n",
    "            print('STOPPING SEARCH')\n",
    "            self.stop_search = True\n",
    "        \n",
    "        print('Depth:', depth)\n",
    "        if (depth == self.max_depth) or (self.stop_search):\n",
    "            return\n",
    "        \n",
    "        # Check if already reached by search\n",
    "        if self.page_hash.get(url, 0) == False:\n",
    "            \n",
    "            # Only sleep if about to request\n",
    "            time.sleep(10)\n",
    "            \n",
    "            # Make http request\n",
    "            try:\n",
    "                html = self.request(url)\n",
    "            except:\n",
    "                return\n",
    "            \n",
    "            print(url)\n",
    "            \n",
    "            # Cache url-html map\n",
    "            self.page_hash[url] = html\n",
    "            \n",
    "            # If html, parse and extract necessary data \n",
    "            if ('/tracklist/' in url):\n",
    "                self.parse(url, html)\n",
    "                    \n",
    "            index = 0\n",
    "            # Iterate over links found in html\n",
    "            while self.find_str(html, 'a href=\"', index) != -1:\n",
    "                \n",
    "                # Extract url\n",
    "                index = self.find_str(html, 'a href=\"', index)\n",
    "                url_chunk = html[index:].split('\"')[1]\n",
    "                \n",
    "                # Make sure it is either a referenced tracklist or 1001 page\n",
    "                if ('/tracklist/' in url_chunk) and\\\n",
    "                   ('http' not in url_chunk) and\\\n",
    "                   ('#tlp' not in url_chunk):\n",
    "                \n",
    "                    self.urls.append(url)\n",
    "                    new_page = 'https://www.1001tracklists.com' + url_chunk\n",
    "                    self.crawl(new_page, depth + 1)\n",
    "                \n",
    "                if ('/dj/' in url_chunk) and\\\n",
    "                   ('http' not in url_chunk) and\\\n",
    "                   ('#tlp' not in url_chunk):\n",
    "                \n",
    "                    self.urls.append(url)\n",
    "                    new_page = 'https://www.1001tracklists.com' + url_chunk\n",
    "                    self.crawl(new_page, depth + 1)\n",
    "                \n",
    "                if ('www.1001tracklists.com' in url_chunk) and\\\n",
    "                   ('#tlp' not in url_chunk) and\\\n",
    "                   ('.xml' not in url_chunk):\n",
    "                    \n",
    "                    self.urls.append(url)\n",
    "                    self.crawl(url_chunk, depth + 1)\n",
    "                    \n",
    "        return\n",
    "        \n",
    "    def start_crawl(self, startUrl):\n",
    "        \n",
    "        depth = 0\n",
    "        self.crawl(startUrl, depth)\n",
    "        \n",
    "crawler = Crawler(max_depth=50)\n",
    "crawler.start_crawl('https://www.1001tracklists.com/source/v7m7k3/the-anjunadeep-edition/index.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parser\n",
    "\n",
    "- Should probably remove features from artist values\n",
    "- and break up artists into multiple entries if there is like & or vs. or something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ftfy\n",
    "import urllib\n",
    "from urllib.request import Request, urlopen\n",
    "\n",
    "def find_str(s, char, start_index=0):\n",
    "    '''\n",
    "    Find substring char in string s. Found on internet, probably not efficient.\n",
    "    \n",
    "    '''\n",
    "    index = 0\n",
    "    s = s[start_index+1:]\n",
    "    if char in s:\n",
    "        c = char[0]\n",
    "        for ch in s:\n",
    "            if ch == c:\n",
    "                if s[index:index+len(char)] == char:\n",
    "                    return start_index + 1 + index\n",
    "            index += 1\n",
    "    return -1\n",
    "\n",
    "def extract_value(html, key_value):\n",
    "    \n",
    "    content_string = html.strip(key_value).split('>')[0].strip('\"')\n",
    "    return content_string\n",
    "\n",
    "def fix_decoding_errors(string):\n",
    "    '''\n",
    "    Fix UTF-8 decoding issues. Probably need to find more systematic/thorough approach to this.\n",
    "    \n",
    "    REPLACE THIS WITH ftfy.fix_text() -- python package which should be one stop shop for fixes\n",
    "    '''\n",
    "    string = string.replace('&amp;','&')\n",
    "    string = string.replace('&#39;',\"'\")\n",
    "    string = string.replace('\\\\xc3\\\\xb6','o')\n",
    "    string = string.replace('\\\\xc3\\\\xab','e')\n",
    "    string = string.replace('\\\\xc3\\\\x9','u')\n",
    "    string = string.replace('\\\\xc3\\\\xb8','o')\n",
    "    string = string.replace(\"\\\\'\",\"'\")\n",
    "    \n",
    "    return ftfy.fix_text(string)\n",
    "\n",
    "def parse_track_and_artist(track_string):\n",
    "    '''\n",
    "    Extract the artist, track name, and remixer (if any) from the standard formatting used by 1001.\n",
    "    \n",
    "    '''\n",
    "    # Check if Remix/Bootleg/Edit and parse accordingly\n",
    "    if ('Remix' in track_string) or ('Bootleg' in track_string) or ('Edit' in track_string):\n",
    "        \n",
    "        artist, track_remixer = [string.strip(' ') for string in track_string.split(' - ')]\n",
    "        track_remixer = [string.strip(' ') for string in track_remixer.split('(')]\n",
    "        \n",
    "        if len(track_remixer) > 2:\n",
    "            track = track_remixer[0]\n",
    "            remixer = '('.join(track_remixer[1:])\n",
    "        else:\n",
    "            track, remixer = track_remixer\n",
    "            remixer = remixer.rstrip('Remix)').strip(' ')\n",
    "        \n",
    "    # If not remix, then should follow standard layout \"Artist Name - Track Name\"\n",
    "    # This layout is expressed explicitly in html\n",
    "    else:\n",
    "        \n",
    "        artist, track = [string.strip(' ') for string in track_string.split(' - ')]\n",
    "        remixer = 'N/A'\n",
    "\n",
    "    # Check for multiple artists -- Big Room sets tend to have hella mashups\n",
    "    # Sometimes there is more structured formatting to exploit i.e. (Artist1 vs. Artist2 - Track1 vs. Track2)\n",
    "    # Not worrying about that now b/c big room sux\n",
    "    if 'vs.' in artist:\n",
    "        artist = artist.replace('vs.','&')\n",
    "    if '&' in artist:\n",
    "        artist = [a.strip(' ') for a in artist.split('&')]\n",
    "    \n",
    "    # Remove features\n",
    "    # We could make features a separate field but for now just removing\n",
    "    if isinstance(artist, str):\n",
    "        if ('feat.' in artist) or ('ft.' in artist):\n",
    "            artist = artist.split('feat.')[0].strip(' ')\n",
    "            artist = artist.split('ft.')[0].strip(' ')\n",
    "    if isinstance(artist, list):\n",
    "            artist = [a.split('feat.')[0].split('ft.')[0].strip(' ') for a in artist]\n",
    "        \n",
    "    if isinstance(artist, list):\n",
    "        return (artist, track, remixer)\n",
    "    else:\n",
    "        return ([artist], track, remixer)\n",
    "\n",
    "\n",
    "def tracklist_meta_data(html):\n",
    "    '''\n",
    "    Extract meta data about tracklist/set.\n",
    "    \n",
    "    '''\n",
    "    meta_data = {}\n",
    "    \n",
    "    # Extract set description\n",
    "    index = 0\n",
    "    start_term = 'meta name=\"description\" content=\"'\n",
    "    index = find_str(html, start_term, index)\n",
    "    description = html[index:].split('>')[0]\n",
    "    description = description.lstrip(start_term).rstrip('\"')\n",
    "    meta_data['description'] = description\n",
    "    \n",
    "    # Set creation date - This should probably be the point in time we use for building prediction data\n",
    "    index = 0\n",
    "    start_term = 'meta name=\"dcterms.created\" content=\"'\n",
    "    index = find_str(html, start_term, index)\n",
    "    created = html[index:].split('>')[0]\n",
    "    created = created.lstrip(start_term).rstrip('\"')\n",
    "    meta_data['created'] = created\n",
    "    \n",
    "    # Set last modified data\n",
    "    index = 0\n",
    "    start_term = 'meta name=\"dcterms.modified\" content=\"'\n",
    "    index = find_str(html, start_term, index)\n",
    "    modified = html[index:].split('>')[0]\n",
    "    modified = modified.lstrip(start_term).rstrip('\"')\n",
    "    meta_data['modified'] = modified\n",
    "    \n",
    "    return meta_data\n",
    "    \n",
    "def tracklist_general_information(html):\n",
    "    '''\n",
    "    Extract general info about tracklist/set.\n",
    "    \n",
    "    '''\n",
    "    info_doc = {}\n",
    "    index = 0\n",
    "    start_term = 'General Information'\n",
    "    index = find_str(html, start_term, index)\n",
    "    info_chunk = html[index:].split('Most Liked Tracklists')[0]\n",
    "    \n",
    "    # Genres -- can use these to build genre-specific graphs\n",
    "    style_index = 0\n",
    "    style_index = find_str(info_chunk, 'Tracklist Musicstyle', style_index)\n",
    "    styles = info_chunk[style_index:].split('id=\"tl_music_styles\">')[1].split('</td>')[0]\n",
    "    styles = [style.strip(' ') for style in styles.split(',')]\n",
    "    info_doc['styles'] = styles\n",
    "    \n",
    "    # If 1001 recognizes the dj who played the set they link their dj page\n",
    "    # Its my understanding dj pages are independent of artist pages -- we'll need to map these\n",
    "    index = 0\n",
    "    start_term = 'a href=\"/dj'\n",
    "    index = find_str(html, start_term, index)\n",
    "    if index != -1:\n",
    "        dj_url = html[index:].split('class')[0].split('\"')[1]\n",
    "        dj_url = 'https://www.1001tracklists.com' + dj_url\n",
    "        info_doc['dj_url'] = dj_url\n",
    "\n",
    "        dj_name = html[index:].split('</a>')[0].split('>')[1]\n",
    "        info_doc['dj_name'] = dj_name\n",
    "    else:\n",
    "        info_doc['dj_url'] = 'N/A'\n",
    "        info_doc['dj_name'] = 'N/A'\n",
    "        \n",
    "    return info_doc\n",
    "    \n",
    "def tracklist_track_data(html):\n",
    "    '''\n",
    "    Extract track related data from set\n",
    "    '''\n",
    "    track_docs = {}\n",
    "    index = 0\n",
    "    while find_str(html, 'tracknumber_value\">', index) != -1:\n",
    "\n",
    "        index = find_str(html, 'tracknumber_value\">', index)\n",
    "        #print(index)\n",
    "        track_chunk = html[index:].split('<br>')[0]\n",
    "        #print(track_chunk)\n",
    "        \n",
    "        # Extract track number\n",
    "        track_num = track_chunk[:22].split('<')[0].strip('tracknumber_value\">')\n",
    "        #print('Track Number:', track_num)\n",
    "\n",
    "        # Extract track information\n",
    "        chunk_index = 0\n",
    "        chunk_index = find_str(track_chunk, 'meta itemprop=\"name\" content=', chunk_index)\n",
    "        extracted_value = track_chunk[chunk_index:].strip('meta itemprop=\"name\" content=').split('>')[0].strip('\"')\n",
    "        clean_string = fix_decoding_errors(extracted_value)\n",
    "        #print(clean_string)\n",
    "        \n",
    "        if len(clean_string) > 1:\n",
    "            try:\n",
    "                artist_list, track, remixer = parse_track_and_artist(clean_string)\n",
    "            except:\n",
    "                artist_list, track, remixer = None, None, None\n",
    "        else:\n",
    "            artist_list, track, remixer = None, None, None\n",
    "            \n",
    "        # Avoid ID's for now\n",
    "        if artist_list is None:\n",
    "            pass\n",
    "        # If track info pull failed then pass\n",
    "        elif (('ID' in artist_list) or ('ID' in track)): \n",
    "            pass\n",
    "        else:\n",
    "            \n",
    "            # Tends to be multiple artists so artists parsed to list even if only one\n",
    "            for artist in artist_list:\n",
    "                \n",
    "                #print('Artist:',artist)\n",
    "                #print('Track:', track)\n",
    "                #print('Remixer:', remixer)\n",
    "\n",
    "                # Extract artist page\n",
    "                artist_index = 0\n",
    "                artist_index = find_str(track_chunk, 'title=\"open artist page\"', artist_index)\n",
    "                if artist_index != -1:\n",
    "                    artist_url = track_chunk[artist_index:].split('class')[1].split('href=\"')[1].rstrip('\" ')\n",
    "                    #print('Aritst url:', artist_url)\n",
    "                    artist_url = 'https://www.1001tracklists.com' + artist_url\n",
    "                else:\n",
    "                    artist_url = 'N/A'\n",
    "\n",
    "                # Extract remixer page (if exists)\n",
    "                if remixer != 'N/A':\n",
    "                    remixer_index = find_str(track_chunk, 'title=\"open remixer artist page\"', artist_index)\n",
    "                    if remixer_index != -1:\n",
    "                        remixer_url = track_chunk[remixer_index:].split('class')[1].split('href=\"')[1].rstrip('\" ')\n",
    "                        #print('Remixer url:', remixer_url)\n",
    "                        remixer_url = 'https://www.1001tracklists.com' + remixer_url\n",
    "                    else:\n",
    "                        remixer_url = 'N/A'\n",
    "                else:\n",
    "                    remixer_url = 'N/A'\n",
    "\n",
    "                # Extract track page\n",
    "                track_index = 0\n",
    "                track_index = find_str(track_chunk, 'title=\"open track page\"', artist_index)\n",
    "                if track_index != -1:\n",
    "                    track_url = track_chunk[track_index:].split('class')[1].split('href=\"')[1].split('\"')[0]\n",
    "                    #print('track url:', track_url)\n",
    "                    track_url = 'https://www.1001tracklists.com' + track_url\n",
    "                else:\n",
    "                    track_url = 'N/A'\n",
    "\n",
    "                track_doc = {\\\n",
    "                            'track_num': track_num,\n",
    "                            'artist': artist.strip(' '),\n",
    "                            'artist_url': artist_url.strip(' '),\n",
    "                            'name': track.strip(' '),\n",
    "                            'track_url': track_url.strip(' '),\n",
    "                            'remixer': remixer.strip(' '),\n",
    "                            'remixer_url': remixer_url.strip(' ')\n",
    "                            }\n",
    "                track_docs[track_num] = track_doc\n",
    "                #print('\\n\\n\\n')\n",
    "\n",
    "    return track_docs\n",
    "\n",
    "def build_artist_edges(url_doc, url):\n",
    "    '''\n",
    "    Build artist set-adjacency docs -- order n^2.\n",
    "    Dont iterate over full set twice since will be considered non-directional\n",
    "    '''\n",
    "    all_tracks = []\n",
    "    these_tracks = list(url_doc['track_docs'].values())\n",
    "    for i in range(len(these_tracks)):\n",
    "        for j in range(i,len(these_tracks)):\n",
    "\n",
    "            track = these_tracks[i]\n",
    "            other_track = these_tracks[j]\n",
    "\n",
    "            first_artist = track['artist']\n",
    "            second_artist = other_track['artist']\n",
    "\n",
    "            if first_artist != second_artist:\n",
    "                all_tracks.append(\\\n",
    "                                {\n",
    "                                'artist1': first_artist,\n",
    "                                'artist2': second_artist,\n",
    "                                'url': url\n",
    "                                }\n",
    "                            )\n",
    "    return all_tracks\n",
    "\n",
    "def build_track_edges(track_docs, url):\n",
    "    '''\n",
    "    Build track set-adjacency docs -- order n^2.\n",
    "    Dont iterate over full set twice since will be considered non-directional\n",
    "    '''\n",
    "    edge_docs = []\n",
    "    keys = sorted(list(track_docs.keys()))\n",
    "    for i in range(len(keys)):\n",
    "        for j in range(i, len(keys)):\n",
    "            \n",
    "            key = keys[i]\n",
    "            other_key = keys[j]\n",
    "            \n",
    "            if key != other_key:\n",
    "                edge_docs.append(\\\n",
    "                                {\n",
    "                                'track1': key,\n",
    "                                'track2': other_key,\n",
    "                                'url': url\n",
    "                                }\n",
    "                            )\n",
    "    return edge_docs\n",
    "                \n",
    "def build_sequential_track_edges(track_docs, url):\n",
    "    '''\n",
    "    Allows for later \"next track lookup\" functionality\n",
    "    \n",
    "    '''\n",
    "    enumerated_tracks = [(track_docs[key]['track_num'], key) for key in list(track_docs.keys())]\n",
    "    enumerated_tracks = sorted(enumerated_tracks, key=lambda x: x[0])\n",
    "    \n",
    "    seq_docs = []\n",
    "    for track_idx in range(len(enumerated_tracks)-1):\n",
    "        seq_docs.append(\\\n",
    "                       {\n",
    "                       'url': url,\n",
    "                       'first_track': enumerated_tracks[track_idx][1],\n",
    "                       'second_track': enumerated_tracks[track_idx+1][1],\n",
    "                       'first_position': enumerated_tracks[track_idx][0],\n",
    "                       'second_position': enumerated_tracks[track_idx+1][0],\n",
    "                       }\n",
    "                    )\n",
    "    return seq_docs\n",
    "\n",
    "def build_played_playedby_edge(url_doc, url):\n",
    "    '''\n",
    "    Allows you to map who plays who.\n",
    "    I think it would be interesting to study directional graphs from this.\n",
    "    \n",
    "    '''\n",
    "    dj_name = url_doc['dj_name']\n",
    "    dj_url = url_doc['dj_url']\n",
    "    \n",
    "    if (dj_name == 'N/A') or (dj_url == 'N/A'):\n",
    "        return []\n",
    "    \n",
    "    played_docs = []\n",
    "    for track_doc in list(url_doc['track_docs'].values()):\n",
    "        \n",
    "        played_docs.append(\\\n",
    "                          {\n",
    "                          'url': url,\n",
    "                          'played_by': dj_name,\n",
    "                          'played_by_url': dj_url,\n",
    "                          'played': track_doc['name'],\n",
    "                          'played_track_url': track_doc['track_url'],\n",
    "                          'played_artist': track_doc['artist'],\n",
    "                          'played_artist_url': track_doc['artist_url'],\n",
    "                          'played_remixer': track_doc['remixer'],\n",
    "                          'played_remixer_url': track_doc['remixer_url']\n",
    "                          }\n",
    "                        )\n",
    "    return played_docs\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spot check parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of track connections: 91\n",
      "Length of Sequential connections: 13\n",
      "Length of played connections: 14\n",
      "Length of artist connections: 85\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import Request, urlopen\n",
    "\n",
    "url = 'https://www.1001tracklists.com/tracklist/2rrcqmpk/modd-vulcan-gas-company-austin-united-states-2019-03-09.html'\n",
    "req = Request(url,\\\n",
    "              headers={'User-Agent': 'Mozilla/5.0'})\n",
    "html = str(urlopen(req).read())\n",
    "\n",
    "url_doc = {}\n",
    "url_doc['html'] = html\n",
    "url_doc.update(tracklist_meta_data(html))\n",
    "url_doc.update(tracklist_general_information(html))\n",
    "track_docs = tracklist_track_data(html)\n",
    "url_doc['track_docs'] = track_docs\n",
    "\n",
    "track_edges = build_track_edges(track_docs, url)\n",
    "print('Length of track connections:', len(track_edges))\n",
    "sequential_edges = build_sequential_track_edges(track_docs, url)\n",
    "print('Length of Sequential connections:', len(sequential_edges))\n",
    "played_edges = build_played_playedby_edge(url_doc, url)\n",
    "print('Length of played connections:', len(played_edges))\n",
    "artist_edges = build_artist_edges(url_doc, url)\n",
    "print('Length of artist connections:', len(artist_edges))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run over these and keep a depth of like 2 for POC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# def find_str(s, char, start_index=0):\n",
    "\n",
    "#     index = 0\n",
    "#     s = s[start_index+1:]\n",
    "#     if char in s:\n",
    "#         c = char[0]\n",
    "#         for ch in s:\n",
    "#             if ch == c:\n",
    "#                 if s[index:index+len(char)] == char:\n",
    "#                     return start_index + 1 + index\n",
    "#             index += 1\n",
    "#     return -1   \n",
    "\n",
    "# def request(url):\n",
    "\n",
    "#     user_agent = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'\n",
    "#     req = Request(url,\\\n",
    "#                   headers={'User-Agent': user_agent})\n",
    "#     html = str(urlopen(req).read())\n",
    "#     return html\n",
    "\n",
    "# date_pages = [\"https://www.1001tracklists.com/index%d.html?order=date\" %d for d in range(100)]\n",
    "# urls = []\n",
    "# for url in date_pages:\n",
    "    \n",
    "#     # Make http request\n",
    "#     html = request(url)\n",
    "#     index = 0\n",
    "#     # Iterate over links found in html\n",
    "#     count = 0\n",
    "#     while find_str(html, 'a href=\"', index) != -1:\n",
    "        \n",
    "#         # Extract url\n",
    "#         index = find_str(html, 'a href=\"', index)\n",
    "#         url_chunk = html[index:].split('\"')[1]\n",
    "\n",
    "#         # Make sure it is either a referenced tracklist or 1001 page\n",
    "#         if ('/tracklist/' in url_chunk) and ('http' not in url_chunk):\n",
    "#             new_page = 'https://www.1001tracklists.com' + url_chunk\n",
    "#             urls.append(new_page)\n",
    "#         if ('www.1001tracklists.com' in url_chunk) and ('.html' in url_chunk):\n",
    "#             urls.append(url_chunk)\n",
    "            \n",
    "#         count += 1\n",
    "            \n",
    "#     print('Number of URLS:', len(urls))\n",
    "#     time.sleep(10)\n",
    "\n",
    "# import pickle\n",
    "# with open('1001_urls.pkl', 'wb') as f:\n",
    "#      pickle.dump(urls, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4534\n",
      "2980\n",
      "2970\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np \n",
    "# import pickle\n",
    "\n",
    "# with open('1001_urls.pkl', 'rb') as f:\n",
    "#      urls = pickle.load(f)\n",
    "        \n",
    "# print(len(urls))\n",
    "# unique_urls = np.unique(urls)\n",
    "# print(len(unique_urls))\n",
    "# unique_urls = [url for url in unique_urls if ('.html' in url) and ('#tlp' not in url)]\n",
    "# print(len(unique_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def parse(url):\n",
    "    \n",
    "    req = Request(url,\\\n",
    "                  headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    html = str(urlopen(req).read())   \n",
    "    \n",
    "    url_doc = {}\n",
    "    url_doc['url'] = url\n",
    "    url_doc['html'] = html\n",
    "    url_doc.update(tracklist_meta_data(html))\n",
    "    url_doc.update(tracklist_general_information(html))\n",
    "\n",
    "    track_docs = tracklist_track_data(html)\n",
    "    url_doc['track_docs'] = track_docs\n",
    "\n",
    "    track_edges = build_track_edges(track_docs, url)\n",
    "    print('Length of track connections:', len(track_edges))\n",
    "    sequential_edges = build_sequential_track_edges(track_docs, url)\n",
    "    print('Length of Sequential connections:', len(sequential_edges))\n",
    "    played_edges = build_played_playedby_edge(url_doc, url)\n",
    "    print('Length of played connections:', len(played_edges))\n",
    "    artist_edges = build_artist_edges(url_doc, url)\n",
    "    print('Length of artist connections:', len(artist_edges))\n",
    "\n",
    "    return url_doc, track_edges, sequential_edges, played_edges, artist_edges, html\n",
    "\n",
    "    \n",
    "played_docs = []\n",
    "sequential_docs = []\n",
    "track_docs = []\n",
    "tracklist_docs = []\n",
    "artist_docs = []\n",
    "url_html_map = {}\n",
    "\n",
    "with open('1001_urls.pkl', 'rb') as f:\n",
    "    urls = pickle.load(f)\n",
    "\n",
    "# Find unique urls\n",
    "seen_urls = []\n",
    "unique_urls = np.unique(urls)\n",
    "unique_urls = [url for url in unique_urls\\\n",
    "                   if ('.html' in url) and ('#tlp' not in url) and (url not in seen_urls)]\n",
    "\n",
    "for url in unique_urls[:]:\n",
    "    \n",
    "    if ('.html' in url) and ('#tlp' not in url) and (url not in seen_urls):\n",
    "    \n",
    "        try:\n",
    "            \n",
    "            url_doc, track_edges, sequential_edges, played_edges, artist_edges, html = parse(url)\n",
    "\n",
    "            url_html_map[url] = html\n",
    "            \n",
    "            played_docs.extend(played_edges)\n",
    "            sequential_docs.extend(sequential_edges)\n",
    "            track_docs.extend(track_edges)\n",
    "            tracklist_docs.append(url_doc)\n",
    "            artist_docs.extend(artist_edges)\n",
    "\n",
    "            print('Len played docs:', len(played_docs))\n",
    "            print('Len sequential docs:', len(sequential_docs))\n",
    "            print('Len track docs:', len(track_docs))\n",
    "            print('Len tracklist docs:', len(tracklist_docs))\n",
    "            print('Len artist docs:', len(artist_docs))\n",
    "            \n",
    "            with open('played_docs3.pkl', 'wb') as f:\n",
    "                pickle.dump(played_docs, f)\n",
    "            with open('sequential_docs3.pkl', 'wb') as f:\n",
    "                pickle.dump(sequential_docs, f)\n",
    "            with open('track_docs3.pkl', 'wb') as f:\n",
    "                pickle.dump(track_docs, f)\n",
    "            with open('tracklist_docs3.pkl', 'wb') as f:\n",
    "                pickle.dump(tracklist_docs, f)\n",
    "            with open('artist_docs3.pkl', 'wb') as f:\n",
    "                pickle.dump(artist_docs, f)\n",
    "            with open('url_html_map.pkl', 'wb') as f:\n",
    "                pickle.dump(artist_docs, f)\n",
    "                \n",
    "            seen_urls.append(url)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        \n",
    "        print('waiting')\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Crawl anjunadeep\n",
    "\n",
    "crawler = Crawler()\n",
    "anjuna_urls = crawler.start_crawl('https://www.1001tracklists.com/source/v7m7k3/the-anjunadeep-edition/index.html')\n",
    "\n",
    "import pickle\n",
    "with open('anjuna_urls.pkl', 'wb') as f:\n",
    "     pickle.dump(anjuna_urls, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def parse(url):\n",
    "    \n",
    "    req = Request(url,\\\n",
    "                  headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    html = str(urlopen(req).read())   \n",
    "    \n",
    "    url_doc = {}\n",
    "    url_doc['url'] = url\n",
    "    url_doc['html'] = html\n",
    "    url_doc.update(tracklist_meta_data(html))\n",
    "    url_doc.update(tracklist_general_information(html))\n",
    "\n",
    "    track_docs = tracklist_track_data(html)\n",
    "    url_doc['track_docs'] = track_docs\n",
    "\n",
    "    track_edges = build_track_edges(track_docs, url)\n",
    "    print('Length of track connections:', len(track_edges))\n",
    "    sequential_edges = build_sequential_track_edges(track_docs, url)\n",
    "    print('Length of Sequential connections:', len(sequential_edges))\n",
    "    played_edges = build_played_playedby_edge(url_doc, url)\n",
    "    print('Length of played connections:', len(played_edges))\n",
    "    artist_edges = build_artist_edges(url_doc, url)\n",
    "    print('Length of artist connections:', len(artist_edges))\n",
    "\n",
    "    return url_doc, track_edges, sequential_edges, played_edges, artist_edges, html\n",
    "\n",
    "    \n",
    "played_docs = []\n",
    "sequential_docs = []\n",
    "track_docs = []\n",
    "tracklist_docs = []\n",
    "artist_docs = []\n",
    "url_html_map = {}\n",
    "\n",
    "with open('1001_urls.pkl', 'rb') as f:\n",
    "    urls = pickle.load(f)\n",
    "\n",
    "# Find unique urls\n",
    "seen_urls = []\n",
    "unique_urls = np.unique(urls)\n",
    "unique_urls = [url for url in unique_urls\\\n",
    "                   if ('.html' in url) and ('#tlp' not in url) and (url not in seen_urls)]\n",
    "\n",
    "for url in unique_urls[:]:\n",
    "    \n",
    "    if ('.html' in url) and ('#tlp' not in url) and (url not in seen_urls):\n",
    "    \n",
    "        try:\n",
    "            \n",
    "            url_doc, track_edges, sequential_edges, played_edges, artist_edges, html = parse(url)\n",
    "\n",
    "            url_html_map[url] = html\n",
    "            \n",
    "            played_docs.extend(played_edges)\n",
    "            sequential_docs.extend(sequential_edges)\n",
    "            track_docs.extend(track_edges)\n",
    "            tracklist_docs.append(url_doc)\n",
    "            artist_docs.extend(artist_edges)\n",
    "\n",
    "            print('Len played docs:', len(played_docs))\n",
    "            print('Len sequential docs:', len(sequential_docs))\n",
    "            print('Len track docs:', len(track_docs))\n",
    "            print('Len tracklist docs:', len(tracklist_docs))\n",
    "            print('Len artist docs:', len(artist_docs))\n",
    "            \n",
    "            with open('played_docs3.pkl', 'wb') as f:\n",
    "                pickle.dump(played_docs, f)\n",
    "            with open('sequential_docs3.pkl', 'wb') as f:\n",
    "                pickle.dump(sequential_docs, f)\n",
    "            with open('track_docs3.pkl', 'wb') as f:\n",
    "                pickle.dump(track_docs, f)\n",
    "            with open('tracklist_docs3.pkl', 'wb') as f:\n",
    "                pickle.dump(tracklist_docs, f)\n",
    "            with open('artist_docs3.pkl', 'wb') as f:\n",
    "                pickle.dump(artist_docs, f)\n",
    "            with open('url_html_map.pkl', 'wb') as f:\n",
    "                pickle.dump(artist_docs, f)\n",
    "                \n",
    "            seen_urls.append(url)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        \n",
    "        print('waiting')\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
