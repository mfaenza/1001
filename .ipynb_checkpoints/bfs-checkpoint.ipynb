{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import queue\n",
    "\n",
    "q = queue.Queue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import pymongo\n",
    "from urllib.request import Request, urlopen\n",
    "import Parser\n",
    "\n",
    "class Crawler:\n",
    "    \n",
    "    def __init__(self, batch_limit=5000):\n",
    "        \n",
    "        # Connect to mongo instance\n",
    "        myclient = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "        db = myclient['1001']\n",
    "        self.url_html_map = db['url_html_map']\n",
    "        \n",
    "        # Instantiate parser\n",
    "        self.Parser = Parser.Parser()\n",
    "\n",
    "        # Instantiate page_hash - maps already found urls to html\n",
    "        # Used to condition on if we've visited a page\n",
    "        self.page_hash = {}\n",
    "        for page in self.url_html_map.find({}):\n",
    "            self.page_hash[page['url']] = 1\n",
    "            \n",
    "        # Stopping indicator\n",
    "        self.stop_search = False\n",
    "        self.batch_limit = batch_limit\n",
    "        \n",
    "        # Var for ensuring loops dont occur\n",
    "        self.urls_visited = {}\n",
    "        \n",
    "        self.last_request = time.time()\n",
    "\n",
    "    def find_str(self, s, char, start_index=0):\n",
    "\n",
    "        index = 0\n",
    "        s = s[start_index+1:]\n",
    "        if char in s:\n",
    "            c = char[0]\n",
    "            for ch in s:\n",
    "                if ch == c:\n",
    "                    if s[index:index+len(char)] == char:\n",
    "                        return start_index + 1 + index\n",
    "                index += 1\n",
    "        return -1\n",
    "\n",
    "    def request(self,url):\n",
    "\n",
    "        req = Request(url,\\\n",
    "                      headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        html = str(urlopen(req).read())\n",
    "        return html\n",
    "    \n",
    "    def request_db(self, url):\n",
    "        \n",
    "        htmls = []\n",
    "        for page in self.url_html_map.find({'url': url}):\n",
    "            htmls.append(page['html'])\n",
    "        \n",
    "        if len(htmls) == 1:\n",
    "            return htmls[0]\n",
    "        if len(htmls) == 0:\n",
    "            return False\n",
    "        if len(htmls) > 1:\n",
    "            return htmls[0]\n",
    "        \n",
    "    def parse(self, url, html): \n",
    "\n",
    "        self.Parser.parse(url, html)\n",
    "    \n",
    "    def crawl(self, url):\n",
    "        \n",
    "        # Insert start url into queue\n",
    "        self.q = queue.Queue()\n",
    "        self.q.put(url)\n",
    "        \n",
    "        # Iterate until queue is empty\n",
    "        while not self.q.empty():\n",
    "            \n",
    "            # Pop from queue (fifo)\n",
    "            url = self.q.get()\n",
    "        \n",
    "            # Check for stopping conditions\n",
    "            if self.Parser.tracklist_num == self.batch_limit:\n",
    "                print('STOPPING SEARCH')\n",
    "                self.stop_search = True\n",
    "            print(url, self.page_hash.get(url, 0))\n",
    "            if (self.stop_search):\n",
    "                return\n",
    "\n",
    "            # If in db then pull html\n",
    "            if (self.page_hash.get(url, 0) == True):\n",
    "\n",
    "                print('requesting db')\n",
    "                html = self.request_db(url)\n",
    "                if html == False:\n",
    "                    print('MORE THAN ONE FOUND FOR:')\n",
    "                    print(url)\n",
    "                    continue\n",
    "\n",
    "            # If not in db then http request\n",
    "            if (self.page_hash.get(url, 0) == False):\n",
    "\n",
    "                print('requesting http')\n",
    "                # Only sleep if gap not enough\n",
    "                if time.time() - self.last_request < 5:\n",
    "                    time.sleep(5 - (time.time() - self.last_request))\n",
    "\n",
    "                # Make http request\n",
    "                try:\n",
    "                    html = self.request(url)\n",
    "                except:\n",
    "                    print('REQUEST FAILED')\n",
    "                    continue\n",
    "                self.last_request = time.time()\n",
    "                \n",
    "                # If http requested then parse and extract necessary data \n",
    "                if ('/tracklist/' in url):\n",
    "                    self.parse(url, html)\n",
    "                    self.page_hash[url] = 1\n",
    "\n",
    "            print('finding links')\n",
    "            # Iterate over links found in html\n",
    "            index = 0\n",
    "            while self.find_str(html, 'a href=\"', index) != -1:\n",
    "\n",
    "                # Extract url\n",
    "                index = self.find_str(html, 'a href=\"', index)\n",
    "                url_chunk = html[index:].split('\"')[1]\n",
    "\n",
    "                # Make sure it is either a referenced tracklist or 1001 page\n",
    "                # and not already reached by search\n",
    "                if ('/tracklist/' in url_chunk) and\\\n",
    "                   ('http' not in url_chunk) and\\\n",
    "                   ('#tlp' not in url_chunk) and\\\n",
    "                   (self.urls_visited.get(url_chunk, 0) == False): # <- compare short address to store from search\n",
    "\n",
    "                    self.urls_visited[url_chunk] = 1 # only shortened address stored\n",
    "                    new_page = 'https://www.1001tracklists.com' + url_chunk\n",
    "                    self.q.put(new_page)\n",
    "\n",
    "                elif ('/dj/' in url_chunk) and\\\n",
    "                     ('http' not in url_chunk) and\\\n",
    "                     ('#tlp' not in url_chunk) and\\\n",
    "                     (self.urls_visited.get(url_chunk, 0) == False):\n",
    "\n",
    "                    self.urls_visited[url_chunk] = 1\n",
    "                    new_page = 'https://www.1001tracklists.com' + url_chunk\n",
    "                    self.q.put(new_page)\n",
    "\n",
    "                elif ('www.1001tracklists.com' in url_chunk) and\\\n",
    "                     ('#tlp' not in url_chunk) and\\\n",
    "                     ('.xml' not in url_chunk) and\\\n",
    "                     (self.urls_visited.get(url_chunk, 0) == False):\n",
    "\n",
    "                    self.urls_visited[url_chunk] = 1\n",
    "                    self.q.put(url_chunk)\n",
    "\n",
    "            # Cache url-html map\n",
    "            self.page_hash[url] = html\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler = Crawler(batch_limit=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crawler.crawl('https://www.1001tracklists.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
